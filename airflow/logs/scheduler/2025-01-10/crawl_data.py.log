[2025-01-10T13:21:08.375+0000] {processor.py:186} INFO - Started process (PID=62) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:21:08.382+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:21:08.384+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:21:08.384+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:21:08.954+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:21:09.611+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:21:09.346+0000] {override.py:1911} INFO - Created Permission View: can delete on DAG:scrape_and_insert_jobs
[2025-01-10T13:21:09.653+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:21:09.653+0000] {override.py:1911} INFO - Created Permission View: can read on DAG:scrape_and_insert_jobs
[2025-01-10T13:21:09.660+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:21:09.660+0000] {override.py:1911} INFO - Created Permission View: can edit on DAG:scrape_and_insert_jobs
[2025-01-10T13:21:09.668+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:21:09.667+0000] {override.py:1911} INFO - Created Permission View: menu access on DAG Run:scrape_and_insert_jobs
[2025-01-10T13:21:09.675+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:21:09.674+0000] {override.py:1911} INFO - Created Permission View: can delete on DAG Run:scrape_and_insert_jobs
[2025-01-10T13:21:09.680+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:21:09.680+0000] {override.py:1911} INFO - Created Permission View: can create on DAG Run:scrape_and_insert_jobs
[2025-01-10T13:21:09.688+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:21:09.687+0000] {override.py:1911} INFO - Created Permission View: can read on DAG Run:scrape_and_insert_jobs
[2025-01-10T13:21:09.689+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:21:09.688+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:21:09.707+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:21:09.707+0000] {dag.py:3262} INFO - Creating ORM DAG for scrape_and_insert_jobs
[2025-01-10T13:21:09.709+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:21:09.709+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:21:09.736+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 1.377 seconds
[2025-01-10T13:21:39.990+0000] {processor.py:186} INFO - Started process (PID=69) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:21:39.990+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:21:39.992+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:21:39.992+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:21:40.150+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:21:40.173+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:21:40.172+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:21:40.186+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:21:40.185+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:21:40.203+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.218 seconds
[2025-01-10T13:22:10.415+0000] {processor.py:186} INFO - Started process (PID=77) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:22:10.416+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:22:10.417+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:22:10.417+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:22:10.577+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:22:10.600+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:22:10.599+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:22:10.732+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:22:10.731+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:22:10.752+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.342 seconds
[2025-01-10T13:22:40.987+0000] {processor.py:186} INFO - Started process (PID=83) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:22:40.988+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:22:40.990+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:22:40.990+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:22:41.160+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:22:41.320+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:22:41.319+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:22:41.331+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:22:41.331+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:22:41.352+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.370 seconds
[2025-01-10T13:23:11.589+0000] {processor.py:186} INFO - Started process (PID=90) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:23:11.590+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:23:11.592+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:23:11.592+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:23:11.872+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:23:11.896+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:23:11.895+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:23:11.908+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:23:11.907+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:23:11.924+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.341 seconds
[2025-01-10T13:23:42.161+0000] {processor.py:186} INFO - Started process (PID=97) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:23:42.162+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:23:42.167+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:23:42.166+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:23:42.897+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:23:42.940+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:23:42.940+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:23:42.964+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:23:42.964+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:23:42.994+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.847 seconds
[2025-01-10T13:24:13.219+0000] {processor.py:186} INFO - Started process (PID=105) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:24:13.220+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:24:13.222+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:24:13.222+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:24:13.530+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:24:13.550+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:24:13.550+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:24:13.562+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:24:13.562+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:24:13.585+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.371 seconds
[2025-01-10T13:24:43.789+0000] {processor.py:186} INFO - Started process (PID=112) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:24:43.790+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:24:43.792+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:24:43.791+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:24:44.083+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:24:44.105+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:24:44.105+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:24:44.117+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:24:44.117+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:24:44.138+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.355 seconds
[2025-01-10T13:25:14.340+0000] {processor.py:186} INFO - Started process (PID=120) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:25:14.341+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:25:14.343+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:25:14.343+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:25:14.619+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:25:14.640+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:25:14.639+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:25:14.651+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:25:14.651+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:25:14.672+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.338 seconds
[2025-01-10T13:25:44.922+0000] {processor.py:186} INFO - Started process (PID=128) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:25:44.925+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:25:44.931+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:25:44.931+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:25:45.198+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:25:45.220+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:25:45.219+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:25:45.232+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:25:45.231+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:25:45.249+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.337 seconds
[2025-01-10T13:26:15.460+0000] {processor.py:186} INFO - Started process (PID=134) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:26:15.461+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:26:15.469+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:26:15.469+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:26:15.765+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:26:15.784+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:26:15.784+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:26:15.797+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:26:15.797+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:26:15.813+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.360 seconds
[2025-01-10T13:26:46.106+0000] {processor.py:186} INFO - Started process (PID=141) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:26:46.108+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:26:46.112+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:26:46.112+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:26:46.656+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:26:46.726+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:26:46.724+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:26:46.760+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:26:46.759+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:26:46.815+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.723 seconds
[2025-01-10T13:27:17.034+0000] {processor.py:186} INFO - Started process (PID=148) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:27:17.034+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:27:17.036+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:27:17.036+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:27:17.280+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:27:17.298+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:27:17.298+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:27:17.309+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:27:17.309+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:27:17.329+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.301 seconds
[2025-01-10T13:27:47.471+0000] {processor.py:186} INFO - Started process (PID=155) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:27:47.472+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:27:47.474+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:27:47.474+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:27:47.732+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:27:47.750+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:27:47.749+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:27:47.760+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:27:47.760+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:27:47.776+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.310 seconds
[2025-01-10T13:28:17.912+0000] {processor.py:186} INFO - Started process (PID=162) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:28:17.913+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:28:17.915+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:28:17.915+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:28:18.172+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:28:18.189+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:28:18.189+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:28:18.201+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:28:18.201+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:28:18.215+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.309 seconds
[2025-01-10T13:28:48.498+0000] {processor.py:186} INFO - Started process (PID=169) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:28:48.498+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:28:48.500+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:28:48.500+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:28:48.745+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:28:48.764+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:28:48.763+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:28:48.774+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:28:48.774+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:28:48.795+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.303 seconds
[2025-01-10T13:29:18.895+0000] {processor.py:186} INFO - Started process (PID=177) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:29:18.896+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:29:18.898+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:29:18.897+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:29:19.144+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:29:19.163+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:29:19.163+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:29:19.174+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:29:19.174+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:29:19.195+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.306 seconds
[2025-01-10T13:29:49.486+0000] {processor.py:186} INFO - Started process (PID=185) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:29:49.487+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:29:49.489+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:29:49.489+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:29:49.740+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:29:49.758+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:29:49.757+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:29:49.769+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:29:49.769+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:29:49.783+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.302 seconds
[2025-01-10T13:30:19.955+0000] {processor.py:186} INFO - Started process (PID=192) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:30:19.956+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:30:19.958+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:30:19.958+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:30:20.241+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:30:20.260+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:30:20.259+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:30:20.273+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:30:20.273+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:30:20.289+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.340 seconds
[2025-01-10T13:30:50.376+0000] {processor.py:186} INFO - Started process (PID=199) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:30:50.377+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:30:50.379+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:30:50.379+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:30:50.675+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:30:50.694+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:30:50.693+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:30:50.706+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:30:50.705+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:30:50.725+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.355 seconds
[2025-01-10T13:31:21.046+0000] {processor.py:186} INFO - Started process (PID=206) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:31:21.046+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:31:21.049+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:31:21.048+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:31:21.379+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:31:21.399+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:31:21.399+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:31:21.412+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:31:21.412+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:31:21.428+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.387 seconds
[2025-01-10T13:31:51.740+0000] {processor.py:186} INFO - Started process (PID=214) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:31:51.741+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:31:51.743+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:31:51.743+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:31:51.987+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:31:52.006+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:31:52.005+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:31:52.016+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:31:52.016+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:31:52.037+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.302 seconds
[2025-01-10T13:32:22.100+0000] {processor.py:186} INFO - Started process (PID=221) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:32:22.101+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:32:22.103+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:32:22.103+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:32:22.360+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:32:22.378+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:32:22.378+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:32:22.390+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:32:22.390+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:32:22.409+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.314 seconds
[2025-01-10T13:32:52.652+0000] {processor.py:186} INFO - Started process (PID=228) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:32:52.653+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:32:52.655+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:32:52.655+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:32:52.905+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:32:52.924+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:32:52.923+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:32:52.934+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:32:52.934+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:32:52.949+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.303 seconds
[2025-01-10T13:33:23.091+0000] {processor.py:186} INFO - Started process (PID=236) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:33:23.091+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:33:23.093+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:33:23.093+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:33:23.360+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:33:23.378+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:33:23.378+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:33:23.388+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:33:23.388+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:33:23.412+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.327 seconds
[2025-01-10T13:33:53.612+0000] {processor.py:186} INFO - Started process (PID=243) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:33:53.613+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:33:53.615+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:33:53.614+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:33:53.862+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:33:53.880+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:33:53.880+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:33:53.890+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:33:53.890+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:33:53.911+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.304 seconds
[2025-01-10T13:34:24.054+0000] {processor.py:186} INFO - Started process (PID=249) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:34:24.055+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:34:24.057+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:34:24.057+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:34:24.307+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:34:24.325+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:34:24.325+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:34:24.337+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:34:24.337+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:34:24.358+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.309 seconds
[2025-01-10T13:34:54.624+0000] {processor.py:186} INFO - Started process (PID=256) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:34:54.625+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:34:54.627+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:34:54.626+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:34:54.869+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:34:54.888+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:34:54.887+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:34:54.898+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:34:54.898+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:34:54.925+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.306 seconds
[2025-01-10T13:35:24.989+0000] {processor.py:186} INFO - Started process (PID=264) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:35:24.990+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:35:24.992+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:35:24.992+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:35:25.245+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:35:25.264+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:35:25.264+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:35:25.275+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:35:25.275+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:35:25.297+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.313 seconds
[2025-01-10T13:35:55.648+0000] {processor.py:186} INFO - Started process (PID=272) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:35:55.649+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:35:55.651+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:35:55.651+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:35:55.898+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:35:55.917+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:35:55.916+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:35:55.927+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:35:55.927+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:35:55.943+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.300 seconds
[2025-01-10T13:36:26.065+0000] {processor.py:186} INFO - Started process (PID=279) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:36:26.066+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:36:26.068+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:36:26.068+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:36:26.314+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:36:26.332+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:36:26.332+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:36:26.344+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:36:26.344+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:36:26.364+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.305 seconds
[2025-01-10T13:36:56.572+0000] {processor.py:186} INFO - Started process (PID=287) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:36:56.573+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:36:56.575+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:36:56.575+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:36:56.823+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:36:56.842+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:36:56.841+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:36:56.852+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:36:56.852+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:36:56.874+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.307 seconds
[2025-01-10T13:37:27.045+0000] {processor.py:186} INFO - Started process (PID=294) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:37:27.046+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:37:27.048+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:37:27.047+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:37:27.294+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:37:27.313+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:37:27.313+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:37:27.324+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:37:27.324+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:37:27.339+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.300 seconds
[2025-01-10T13:37:57.459+0000] {processor.py:186} INFO - Started process (PID=301) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:37:57.460+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:37:57.462+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:37:57.462+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:37:57.711+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:37:57.729+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:37:57.729+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:37:57.740+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:37:57.740+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:37:57.762+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.308 seconds
[2025-01-10T13:38:27.905+0000] {processor.py:186} INFO - Started process (PID=308) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:38:27.905+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:38:27.908+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:38:27.907+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:38:28.234+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:38:28.254+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:38:28.253+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:38:28.266+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:38:28.266+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:38:28.303+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.405 seconds
[2025-01-10T13:38:58.554+0000] {processor.py:186} INFO - Started process (PID=323) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:38:58.555+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:38:58.557+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:38:58.556+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:38:58.804+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:38:58.823+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:38:58.822+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:38:58.833+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:38:58.833+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:38:58.855+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.306 seconds
[2025-01-10T13:39:29.033+0000] {processor.py:186} INFO - Started process (PID=330) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:39:29.033+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:39:29.036+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:39:29.035+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:39:29.295+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:39:29.313+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:39:29.313+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:39:29.324+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:39:29.324+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:39:29.339+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.312 seconds
[2025-01-10T13:39:59.559+0000] {processor.py:186} INFO - Started process (PID=338) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:39:59.560+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:39:59.563+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:39:59.562+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:39:59.906+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:39:59.928+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:39:59.928+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:39:59.941+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:39:59.941+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:39:59.958+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.406 seconds
[2025-01-10T13:40:30.326+0000] {processor.py:186} INFO - Started process (PID=344) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:40:30.326+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:40:30.328+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:40:30.328+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:40:30.579+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:40:30.598+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:40:30.597+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:40:30.609+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:40:30.609+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:40:30.631+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.311 seconds
[2025-01-10T13:41:01.035+0000] {processor.py:186} INFO - Started process (PID=351) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:41:01.036+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:41:01.038+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:41:01.038+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:41:01.284+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:41:01.303+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:41:01.302+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:41:01.313+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:41:01.313+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:41:01.335+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.304 seconds
[2025-01-10T13:41:31.420+0000] {processor.py:186} INFO - Started process (PID=358) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:41:31.421+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:41:31.423+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:41:31.423+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:41:31.674+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:41:31.692+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:41:31.692+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:41:31.703+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:41:31.703+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:41:31.719+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.304 seconds
[2025-01-10T13:42:01.954+0000] {processor.py:186} INFO - Started process (PID=365) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:42:01.955+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:42:01.957+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:42:01.957+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:42:02.215+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:42:02.234+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:42:02.233+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:42:02.245+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:42:02.245+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:42:02.259+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.311 seconds
[2025-01-10T13:42:32.617+0000] {processor.py:186} INFO - Started process (PID=372) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:42:32.617+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:42:32.619+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:42:32.619+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:42:32.866+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:42:32.886+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:42:32.885+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:42:32.898+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:42:32.898+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:42:32.914+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.303 seconds
[2025-01-10T13:43:02.986+0000] {processor.py:186} INFO - Started process (PID=379) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:43:02.986+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:43:02.988+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:43:02.988+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:43:03.240+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:43:03.258+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:43:03.258+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:43:03.269+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:43:03.269+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:43:03.291+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.311 seconds
[2025-01-10T13:43:33.603+0000] {processor.py:186} INFO - Started process (PID=386) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:43:33.604+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:43:33.606+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:43:33.606+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:43:33.852+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:43:33.870+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:43:33.870+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:43:33.881+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:43:33.881+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:43:33.896+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.298 seconds
[2025-01-10T13:44:04.003+0000] {processor.py:186} INFO - Started process (PID=394) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:44:04.004+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:44:04.006+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:44:04.006+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:44:04.254+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:44:04.272+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:44:04.272+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:44:04.283+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:44:04.282+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:44:04.298+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.301 seconds
[2025-01-10T13:44:34.690+0000] {processor.py:186} INFO - Started process (PID=401) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:44:34.691+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:44:34.693+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:44:34.692+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:44:34.831+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:44:34.852+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:44:34.851+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:44:34.865+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:44:34.865+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:44:34.882+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.197 seconds
[2025-01-10T13:45:05.056+0000] {processor.py:186} INFO - Started process (PID=408) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:45:05.057+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:45:05.059+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:45:05.059+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:45:05.201+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:45:05.223+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:45:05.222+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:45:05.235+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:45:05.235+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:45:05.251+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T13:45:35.482+0000] {processor.py:186} INFO - Started process (PID=414) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:45:35.483+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:45:35.485+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:45:35.485+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:45:35.625+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:45:35.646+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:45:35.645+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:45:35.658+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:45:35.658+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:45:35.674+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.198 seconds
[2025-01-10T13:46:06.037+0000] {processor.py:186} INFO - Started process (PID=420) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:46:06.038+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:46:06.040+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:46:06.039+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:46:06.179+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:46:06.199+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:46:06.199+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:46:06.212+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:46:06.212+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:46:06.233+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T13:46:36.365+0000] {processor.py:186} INFO - Started process (PID=428) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:46:36.366+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:46:36.370+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:46:36.370+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:46:36.528+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:46:36.551+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:46:36.550+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:46:36.566+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:46:36.566+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:46:36.584+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.225 seconds
[2025-01-10T13:47:06.993+0000] {processor.py:186} INFO - Started process (PID=436) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:47:06.994+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:47:06.996+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:47:06.996+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:47:07.138+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:47:07.161+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:47:07.160+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:47:07.193+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:47:07.192+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:47:07.232+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.244 seconds
[2025-01-10T13:47:37.682+0000] {processor.py:186} INFO - Started process (PID=443) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:47:37.683+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:47:37.684+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:47:37.684+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:47:37.824+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:47:37.847+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:47:37.846+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:47:37.859+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:47:37.859+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:47:37.875+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.199 seconds
[2025-01-10T13:48:08.049+0000] {processor.py:186} INFO - Started process (PID=450) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:48:08.050+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:48:08.052+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:48:08.052+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:48:08.192+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:48:08.213+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:48:08.213+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:48:08.226+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:48:08.226+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:48:08.249+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T13:48:38.571+0000] {processor.py:186} INFO - Started process (PID=457) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:48:38.572+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:48:38.575+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:48:38.574+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:48:38.713+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:48:38.734+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:48:38.733+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:48:38.747+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:48:38.747+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:48:38.762+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.197 seconds
[2025-01-10T13:49:09.006+0000] {processor.py:186} INFO - Started process (PID=464) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:49:09.007+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:49:09.009+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:49:09.009+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:49:09.172+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:49:09.196+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:49:09.196+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:49:09.211+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:49:09.211+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:49:09.245+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.245 seconds
[2025-01-10T13:49:39.377+0000] {processor.py:186} INFO - Started process (PID=472) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:49:39.378+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:49:39.380+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:49:39.380+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:49:39.519+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:49:39.541+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:49:39.541+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:49:39.554+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:49:39.554+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:49:39.571+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T13:50:09.734+0000] {processor.py:186} INFO - Started process (PID=480) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:50:09.735+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:50:09.737+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:50:09.736+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:50:09.876+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:50:09.898+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:50:09.898+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:50:09.911+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:50:09.910+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:50:09.934+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T13:50:40.075+0000] {processor.py:186} INFO - Started process (PID=487) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:50:40.076+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:50:40.078+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:50:40.078+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:50:40.254+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:50:40.285+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:50:40.285+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:50:40.308+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:50:40.308+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:50:40.325+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.255 seconds
[2025-01-10T13:51:10.668+0000] {processor.py:186} INFO - Started process (PID=494) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:51:10.669+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:51:10.671+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:51:10.670+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:51:10.810+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:51:10.833+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:51:10.833+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:51:10.847+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:51:10.846+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:51:10.868+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T13:51:41.001+0000] {processor.py:186} INFO - Started process (PID=501) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:51:41.002+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:51:41.006+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:51:41.005+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:51:41.147+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:51:41.169+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:51:41.169+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:51:41.182+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:51:41.182+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:51:41.198+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T13:52:11.577+0000] {processor.py:186} INFO - Started process (PID=508) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:52:11.578+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:52:11.580+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:52:11.580+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:52:11.720+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:52:11.741+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:52:11.741+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:52:11.754+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:52:11.754+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:52:11.777+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T13:52:41.951+0000] {processor.py:186} INFO - Started process (PID=515) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:52:41.952+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:52:41.954+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:52:41.954+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:52:42.093+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:52:42.114+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:52:42.114+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:52:42.128+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:52:42.127+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:52:42.150+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T13:53:12.348+0000] {processor.py:186} INFO - Started process (PID=523) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:53:12.349+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:53:12.351+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:53:12.351+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:53:12.491+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:53:12.513+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:53:12.513+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:53:12.526+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:53:12.526+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:53:12.543+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T13:53:42.733+0000] {processor.py:186} INFO - Started process (PID=530) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:53:42.733+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:53:42.736+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:53:42.735+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:53:42.878+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:53:42.899+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:53:42.899+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:53:42.912+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:53:42.912+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:53:42.929+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T13:54:13.179+0000] {processor.py:186} INFO - Started process (PID=537) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:54:13.179+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:54:13.181+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:54:13.181+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:54:13.323+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:54:13.343+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:54:13.342+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:54:13.355+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:54:13.355+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:54:13.377+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T13:54:43.685+0000] {processor.py:186} INFO - Started process (PID=544) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:54:43.685+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:54:43.688+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:54:43.688+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:54:43.842+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:54:43.868+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:54:43.867+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:54:43.882+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:54:43.881+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:54:43.908+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.228 seconds
[2025-01-10T13:55:14.153+0000] {processor.py:186} INFO - Started process (PID=552) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:55:14.154+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:55:14.156+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:55:14.156+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:55:14.309+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:55:14.330+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:55:14.330+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:55:14.342+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:55:14.342+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:55:14.359+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.211 seconds
[2025-01-10T13:55:44.653+0000] {processor.py:186} INFO - Started process (PID=559) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:55:44.654+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:55:44.656+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:55:44.656+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:55:44.794+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:55:44.815+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:55:44.814+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:55:44.827+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:55:44.827+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:55:44.842+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.194 seconds
[2025-01-10T13:56:15.115+0000] {processor.py:186} INFO - Started process (PID=567) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:56:15.116+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:56:15.118+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:56:15.117+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:56:15.257+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:56:15.279+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:56:15.278+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:56:15.291+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:56:15.291+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:56:15.314+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T13:56:45.544+0000] {processor.py:186} INFO - Started process (PID=574) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:56:45.545+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:56:45.547+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:56:45.547+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:56:45.690+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:56:45.712+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:56:45.711+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:56:45.725+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:56:45.725+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:56:45.748+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.211 seconds
[2025-01-10T13:57:15.876+0000] {processor.py:186} INFO - Started process (PID=581) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:57:15.876+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:57:15.879+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:57:15.878+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:57:16.017+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:57:16.040+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:57:16.039+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:57:16.053+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:57:16.053+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:57:16.071+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T13:57:46.173+0000] {processor.py:186} INFO - Started process (PID=589) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:57:46.180+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:57:46.186+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:57:46.185+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:57:46.341+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:57:46.363+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:57:46.362+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:57:46.377+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:57:46.377+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:57:46.393+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.226 seconds
[2025-01-10T13:58:16.475+0000] {processor.py:186} INFO - Started process (PID=596) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:58:16.476+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:58:16.479+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:58:16.478+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:58:16.623+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:58:16.645+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:58:16.644+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:58:16.658+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:58:16.657+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:58:16.674+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T13:58:46.810+0000] {processor.py:186} INFO - Started process (PID=603) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:58:46.811+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:58:46.813+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:58:46.813+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:58:46.952+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:58:46.975+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:58:46.975+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:58:46.989+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:58:46.989+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:58:47.008+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T13:59:17.087+0000] {processor.py:186} INFO - Started process (PID=610) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:59:17.088+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:59:17.090+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:59:17.090+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:59:17.227+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:59:17.250+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:59:17.249+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:59:17.262+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:59:17.262+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:59:17.278+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.197 seconds
[2025-01-10T13:59:47.627+0000] {processor.py:186} INFO - Started process (PID=617) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T13:59:47.628+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T13:59:47.630+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:59:47.630+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:59:47.770+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T13:59:47.792+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:59:47.792+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T13:59:47.804+0000] {logging_mixin.py:190} INFO - [2025-01-10T13:59:47.804+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T13:59:47.827+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T14:00:17.928+0000] {processor.py:186} INFO - Started process (PID=625) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:00:17.929+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:00:17.931+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:00:17.931+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:00:18.071+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:00:18.093+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:00:18.093+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:00:18.106+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:00:18.105+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:00:18.123+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T14:00:48.199+0000] {processor.py:186} INFO - Started process (PID=632) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:00:48.200+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:00:48.202+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:00:48.201+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:00:48.342+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:00:48.364+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:00:48.364+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:00:48.378+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:00:48.378+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:00:48.402+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T14:01:18.567+0000] {processor.py:186} INFO - Started process (PID=639) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:01:18.568+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:01:18.570+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:01:18.569+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:01:18.707+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:01:18.729+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:01:18.728+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:01:18.741+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:01:18.741+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:01:18.763+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T14:01:49.103+0000] {processor.py:186} INFO - Started process (PID=647) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:01:49.104+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:01:49.106+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:01:49.106+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:01:49.248+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:01:49.269+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:01:49.269+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:01:49.282+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:01:49.282+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:01:49.304+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T14:02:19.437+0000] {processor.py:186} INFO - Started process (PID=654) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:02:19.438+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:02:19.441+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:02:19.440+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:02:19.605+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:02:19.647+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:02:19.647+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:02:19.662+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:02:19.662+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:02:19.682+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.250 seconds
[2025-01-10T14:02:49.989+0000] {processor.py:186} INFO - Started process (PID=662) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:02:49.990+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:02:49.992+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:02:49.991+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:02:50.131+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:02:50.152+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:02:50.151+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:02:50.165+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:02:50.165+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:02:50.200+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.216 seconds
[2025-01-10T14:03:20.361+0000] {processor.py:186} INFO - Started process (PID=670) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:03:20.362+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:03:20.364+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:03:20.364+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:03:20.503+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:03:20.525+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:03:20.524+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:03:20.537+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:03:20.537+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:03:20.560+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T14:03:50.843+0000] {processor.py:186} INFO - Started process (PID=677) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:03:50.844+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:03:50.846+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:03:50.846+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:03:50.986+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:03:51.007+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:03:51.006+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:03:51.019+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:03:51.019+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:03:51.041+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T14:04:21.142+0000] {processor.py:186} INFO - Started process (PID=684) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:04:21.142+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:04:21.145+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:04:21.144+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:04:21.283+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:04:21.305+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:04:21.305+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:04:21.318+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:04:21.318+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:04:21.334+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.198 seconds
[2025-01-10T14:04:51.705+0000] {processor.py:186} INFO - Started process (PID=690) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:04:51.706+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:04:51.708+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:04:51.707+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:04:51.844+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:04:51.865+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:04:51.865+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:04:51.878+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:04:51.878+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:04:51.900+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T14:05:22.056+0000] {processor.py:186} INFO - Started process (PID=697) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:05:22.057+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:05:22.059+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:05:22.059+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:05:22.198+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:05:22.221+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:05:22.220+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:05:22.233+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:05:22.233+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:05:22.256+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T14:05:52.640+0000] {processor.py:186} INFO - Started process (PID=704) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:05:52.641+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:05:52.643+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:05:52.643+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:05:52.796+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:05:52.817+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:05:52.817+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:05:52.830+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:05:52.830+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:05:52.852+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.218 seconds
[2025-01-10T14:06:23.036+0000] {processor.py:186} INFO - Started process (PID=712) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:06:23.037+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:06:23.039+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:06:23.039+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:06:23.205+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:06:23.227+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:06:23.226+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:06:23.241+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:06:23.240+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:06:23.257+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.228 seconds
[2025-01-10T14:06:53.541+0000] {processor.py:186} INFO - Started process (PID=719) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:06:53.542+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:06:53.544+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:06:53.544+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:06:53.685+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:06:53.706+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:06:53.706+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:06:53.720+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:06:53.719+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:06:53.743+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T14:07:23.871+0000] {processor.py:186} INFO - Started process (PID=726) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:07:23.872+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:07:23.874+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:07:23.874+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:07:24.013+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:07:24.036+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:07:24.035+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:07:24.048+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:07:24.048+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:07:24.069+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T14:07:54.381+0000] {processor.py:186} INFO - Started process (PID=733) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:07:54.382+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:07:54.384+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:07:54.384+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:07:54.521+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:07:54.543+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:07:54.542+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:07:54.555+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:07:54.555+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:07:54.579+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T14:08:24.775+0000] {processor.py:186} INFO - Started process (PID=740) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:08:24.776+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:08:24.778+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:08:24.778+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:08:24.921+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:08:24.943+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:08:24.942+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:08:24.955+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:08:24.955+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:08:24.971+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T14:08:55.383+0000] {processor.py:186} INFO - Started process (PID=748) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:08:55.384+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:08:55.386+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:08:55.386+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:08:55.525+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:08:55.546+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:08:55.546+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:08:55.560+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:08:55.560+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:08:55.584+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T14:09:25.729+0000] {processor.py:186} INFO - Started process (PID=756) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:09:25.730+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:09:25.732+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:09:25.731+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:09:25.871+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:09:25.893+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:09:25.893+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:09:25.907+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:09:25.906+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:09:25.922+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.199 seconds
[2025-01-10T14:09:56.232+0000] {processor.py:186} INFO - Started process (PID=764) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:09:56.232+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:09:56.235+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:09:56.234+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:09:56.372+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:09:56.393+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:09:56.393+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:09:56.405+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:09:56.405+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:09:56.422+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.195 seconds
[2025-01-10T14:10:26.532+0000] {processor.py:186} INFO - Started process (PID=772) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:10:26.533+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:10:26.535+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:10:26.535+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:10:26.695+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:10:26.716+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:10:26.715+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:10:26.729+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:10:26.729+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:10:26.745+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.230 seconds
[2025-01-10T14:10:56.824+0000] {processor.py:186} INFO - Started process (PID=779) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:10:56.825+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:10:56.827+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:10:56.827+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:10:56.970+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:10:56.992+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:10:56.992+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:10:57.008+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:10:57.008+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:10:57.025+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T14:11:27.130+0000] {processor.py:186} INFO - Started process (PID=786) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:11:27.131+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:11:27.133+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:11:27.132+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:11:27.270+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:11:27.292+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:11:27.292+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:11:27.305+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:11:27.305+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:11:27.321+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.196 seconds
[2025-01-10T14:11:57.673+0000] {processor.py:186} INFO - Started process (PID=793) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:11:57.673+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:11:57.676+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:11:57.675+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:11:57.814+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:11:57.835+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:11:57.835+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:11:57.848+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:11:57.848+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:11:57.871+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T14:12:28.041+0000] {processor.py:186} INFO - Started process (PID=800) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:12:28.041+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:12:28.044+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:12:28.043+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:12:28.184+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:12:28.205+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:12:28.204+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:12:28.217+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:12:28.217+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:12:28.240+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T14:12:58.607+0000] {processor.py:186} INFO - Started process (PID=808) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:12:58.608+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:12:58.610+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:12:58.610+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:12:58.751+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:12:58.772+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:12:58.772+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:12:58.785+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:12:58.785+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:12:58.803+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T14:13:28.934+0000] {processor.py:186} INFO - Started process (PID=815) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:13:28.935+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:13:28.937+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:13:28.936+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:13:29.082+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:13:29.105+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:13:29.105+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:13:29.118+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:13:29.117+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:13:29.139+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.211 seconds
[2025-01-10T14:13:59.510+0000] {processor.py:186} INFO - Started process (PID=821) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:13:59.511+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:13:59.514+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:13:59.513+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:13:59.657+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:13:59.679+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:13:59.678+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:13:59.692+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:13:59.691+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:13:59.707+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T14:14:29.844+0000] {processor.py:186} INFO - Started process (PID=829) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:14:29.845+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:14:29.848+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:14:29.847+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:14:30.011+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:14:30.032+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:14:30.032+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:14:30.047+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:14:30.046+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:14:30.063+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.230 seconds
[2025-01-10T14:15:00.462+0000] {processor.py:186} INFO - Started process (PID=836) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:15:00.463+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:15:00.466+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:15:00.465+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:15:00.702+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:15:00.725+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:15:00.724+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:15:00.740+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:15:00.740+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:15:00.763+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.306 seconds
[2025-01-10T14:15:31.166+0000] {processor.py:186} INFO - Started process (PID=848) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:15:31.167+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:15:31.169+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:15:31.169+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:15:31.308+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:15:31.331+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:15:31.330+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:15:31.343+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:15:31.343+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:15:31.361+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T14:16:01.513+0000] {processor.py:186} INFO - Started process (PID=855) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:16:01.514+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:16:01.516+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:16:01.516+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:16:01.657+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:16:01.680+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:16:01.679+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:16:01.693+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:16:01.693+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:16:01.714+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T14:16:32.053+0000] {processor.py:186} INFO - Started process (PID=862) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:16:32.054+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:16:32.056+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:16:32.056+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:16:32.196+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:16:32.217+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:16:32.217+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:16:32.230+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:16:32.230+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:16:32.253+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T14:17:02.331+0000] {processor.py:186} INFO - Started process (PID=869) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:17:02.332+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:17:02.334+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:17:02.334+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:17:02.473+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:17:02.496+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:17:02.495+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:17:02.510+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:17:02.509+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:17:02.531+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T14:17:32.585+0000] {processor.py:186} INFO - Started process (PID=876) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:17:32.586+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:17:32.588+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:17:32.587+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:17:32.728+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:17:32.751+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:17:32.750+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:17:32.763+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:17:32.763+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:17:32.780+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T14:18:02.934+0000] {processor.py:186} INFO - Started process (PID=884) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:18:02.935+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:18:02.937+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:18:02.937+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:18:03.113+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:18:03.136+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:18:03.135+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:18:03.150+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:18:03.150+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:18:03.169+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.241 seconds
[2025-01-10T14:18:33.477+0000] {processor.py:186} INFO - Started process (PID=892) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:18:33.478+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:18:33.480+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:18:33.480+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:18:33.637+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:18:33.659+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:18:33.659+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:18:33.675+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:18:33.675+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:18:33.692+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.221 seconds
[2025-01-10T14:19:03.858+0000] {processor.py:186} INFO - Started process (PID=900) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:19:03.859+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:19:03.861+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:19:03.861+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:19:04.008+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:19:04.029+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:19:04.029+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:19:04.042+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:19:04.042+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:19:04.059+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T14:19:34.318+0000] {processor.py:186} INFO - Started process (PID=906) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:19:34.319+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:19:34.321+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:19:34.321+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:19:34.472+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:19:34.504+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:19:34.504+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:19:34.518+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:19:34.518+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:19:34.537+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.224 seconds
[2025-01-10T14:20:04.723+0000] {processor.py:186} INFO - Started process (PID=913) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:20:04.724+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:20:04.726+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:20:04.726+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:20:04.865+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:20:04.886+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:20:04.885+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:20:04.899+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:20:04.898+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:20:04.919+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T14:20:35.300+0000] {processor.py:186} INFO - Started process (PID=920) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:20:35.301+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:20:35.303+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:20:35.303+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:20:35.441+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:20:35.464+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:20:35.463+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:20:35.479+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:20:35.478+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:20:35.497+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T14:21:05.713+0000] {processor.py:186} INFO - Started process (PID=927) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:21:05.714+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:21:05.716+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:21:05.716+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:21:05.856+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:21:05.877+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:21:05.877+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:21:05.891+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:21:05.891+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:21:05.908+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T14:21:36.114+0000] {processor.py:186} INFO - Started process (PID=934) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:21:36.115+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:21:36.117+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:21:36.117+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:21:36.256+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:21:36.278+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:21:36.277+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:21:36.290+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:21:36.290+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:21:36.312+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T14:22:06.526+0000] {processor.py:186} INFO - Started process (PID=940) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:22:06.527+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:22:06.529+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:22:06.529+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:22:06.670+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:22:06.695+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:22:06.695+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:22:06.708+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:22:06.708+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:22:06.731+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.210 seconds
[2025-01-10T14:22:37.027+0000] {processor.py:186} INFO - Started process (PID=947) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:22:37.028+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:22:37.030+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:22:37.029+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:22:37.180+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:22:37.201+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:22:37.200+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:22:37.214+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:22:37.214+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:22:37.230+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T14:23:07.422+0000] {processor.py:186} INFO - Started process (PID=955) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:23:07.425+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:23:07.431+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:23:07.430+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:23:07.609+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:23:07.633+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:23:07.633+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:23:07.647+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:23:07.647+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:23:07.668+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.272 seconds
[2025-01-10T14:23:37.878+0000] {processor.py:186} INFO - Started process (PID=962) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:23:37.879+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:23:37.881+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:23:37.881+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:23:38.044+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:23:38.066+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:23:38.065+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:23:38.080+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:23:38.079+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:23:38.096+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.224 seconds
[2025-01-10T14:24:08.364+0000] {processor.py:186} INFO - Started process (PID=969) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:24:08.365+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:24:08.367+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:24:08.367+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:24:08.505+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:24:08.537+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:24:08.537+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:24:08.550+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:24:08.550+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:24:08.573+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.214 seconds
[2025-01-10T14:24:38.868+0000] {processor.py:186} INFO - Started process (PID=977) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:24:38.869+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:24:38.872+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:24:38.872+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:24:39.014+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:24:39.035+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:24:39.034+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:24:39.047+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:24:39.047+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:24:39.064+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T14:25:09.180+0000] {processor.py:186} INFO - Started process (PID=984) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:25:09.181+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:25:09.183+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:25:09.183+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:25:09.324+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:25:09.346+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:25:09.346+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:25:09.360+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:25:09.359+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:25:09.378+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T14:25:39.490+0000] {processor.py:186} INFO - Started process (PID=991) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:25:39.491+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:25:39.493+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:25:39.493+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:25:39.634+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:25:39.656+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:25:39.656+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:25:39.669+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:25:39.669+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:25:39.685+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T14:26:09.857+0000] {processor.py:186} INFO - Started process (PID=999) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:26:09.858+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:26:09.860+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:26:09.859+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:26:10.019+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:26:10.042+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:26:10.042+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:26:10.055+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:26:10.055+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:26:10.073+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.222 seconds
[2025-01-10T14:26:40.415+0000] {processor.py:186} INFO - Started process (PID=1007) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:26:40.415+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:26:40.417+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:26:40.417+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:26:40.560+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:26:40.583+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:26:40.583+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:26:40.596+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:26:40.596+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:26:40.618+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T14:27:10.821+0000] {processor.py:186} INFO - Started process (PID=1014) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:27:10.822+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:27:10.824+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:27:10.824+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:27:10.966+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:27:10.989+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:27:10.989+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:27:11.003+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:27:11.003+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:27:11.020+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T14:27:41.353+0000] {processor.py:186} INFO - Started process (PID=1021) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:27:41.354+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:27:41.356+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:27:41.355+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:27:41.492+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:27:41.514+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:27:41.513+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:27:41.527+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:27:41.527+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:27:41.550+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T14:28:11.740+0000] {processor.py:186} INFO - Started process (PID=1029) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:28:11.741+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:28:11.743+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:28:11.743+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:28:11.881+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:28:11.902+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:28:11.902+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:28:11.916+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:28:11.915+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:28:11.932+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.198 seconds
[2025-01-10T14:28:42.161+0000] {processor.py:186} INFO - Started process (PID=1036) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:28:42.162+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:28:42.165+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:28:42.164+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:28:42.306+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:28:42.327+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:28:42.327+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:28:42.340+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:28:42.340+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:28:42.364+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T14:29:12.587+0000] {processor.py:186} INFO - Started process (PID=1042) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:29:12.588+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:29:12.590+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:29:12.590+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:29:12.730+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:29:12.752+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:29:12.751+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:29:12.765+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:29:12.765+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:29:12.789+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T14:29:43.035+0000] {processor.py:186} INFO - Started process (PID=1050) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:29:43.036+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:29:43.038+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:29:43.038+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:29:43.178+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:29:43.200+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:29:43.200+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:29:43.213+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:29:43.213+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:29:43.229+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.199 seconds
[2025-01-10T14:30:13.441+0000] {processor.py:186} INFO - Started process (PID=1057) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:30:13.442+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:30:13.444+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:30:13.443+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:30:13.584+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:30:13.605+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:30:13.604+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:30:13.617+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:30:13.617+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:30:13.639+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T14:30:43.956+0000] {processor.py:186} INFO - Started process (PID=1064) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:30:43.957+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:30:43.959+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:30:43.959+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:30:44.100+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:30:44.121+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:30:44.120+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:30:44.133+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:30:44.133+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:30:44.166+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.215 seconds
[2025-01-10T14:31:14.328+0000] {processor.py:186} INFO - Started process (PID=1071) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:31:14.329+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:31:14.331+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:31:14.330+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:31:14.470+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:31:14.493+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:31:14.492+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:31:14.507+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:31:14.506+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:31:14.530+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T14:31:44.882+0000] {processor.py:186} INFO - Started process (PID=1078) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:31:44.883+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:31:44.885+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:31:44.885+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:31:45.036+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:31:45.058+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:31:45.057+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:31:45.070+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:31:45.070+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:31:45.088+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.211 seconds
[2025-01-10T14:32:15.209+0000] {processor.py:186} INFO - Started process (PID=1085) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:32:15.210+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:32:15.213+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:32:15.212+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:32:15.389+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:32:15.431+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:32:15.431+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:32:15.452+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:32:15.452+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:32:15.470+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.268 seconds
[2025-01-10T14:32:45.787+0000] {processor.py:186} INFO - Started process (PID=1092) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:32:45.788+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:32:45.790+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:32:45.790+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:32:45.930+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:32:45.952+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:32:45.951+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:32:45.966+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:32:45.966+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:32:45.984+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T14:33:16.084+0000] {processor.py:186} INFO - Started process (PID=1099) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:33:16.085+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:33:16.087+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:33:16.087+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:33:16.227+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:33:16.249+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:33:16.248+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:33:16.262+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:33:16.262+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:33:16.286+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T14:33:46.396+0000] {processor.py:186} INFO - Started process (PID=1106) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:33:46.397+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:33:46.401+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:33:46.401+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:33:46.552+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:33:46.575+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:33:46.575+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:33:46.589+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:33:46.588+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:33:46.605+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.219 seconds
[2025-01-10T14:34:16.711+0000] {processor.py:186} INFO - Started process (PID=1113) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:34:16.712+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:34:16.714+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:34:16.713+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:34:16.863+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:34:16.884+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:34:16.884+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:34:16.898+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:34:16.897+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:34:16.915+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T14:34:47.042+0000] {processor.py:186} INFO - Started process (PID=1121) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:34:47.043+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:34:47.045+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:34:47.045+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:34:47.187+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:34:47.209+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:34:47.208+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:34:47.222+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:34:47.222+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:34:47.240+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T14:35:17.671+0000] {processor.py:186} INFO - Started process (PID=1128) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:35:17.672+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:35:17.674+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:35:17.673+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:35:17.813+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:35:17.835+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:35:17.834+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:35:17.848+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:35:17.848+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:35:17.871+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T14:35:48.032+0000] {processor.py:186} INFO - Started process (PID=1135) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:35:48.033+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:35:48.035+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:35:48.035+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:35:48.171+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:35:48.193+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:35:48.192+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:35:48.205+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:35:48.205+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:35:48.229+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T14:36:18.592+0000] {processor.py:186} INFO - Started process (PID=1142) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:36:18.592+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:36:18.595+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:36:18.594+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:36:18.732+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:36:18.755+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:36:18.755+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:36:18.768+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:36:18.767+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:36:18.784+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.197 seconds
[2025-01-10T14:36:48.993+0000] {processor.py:186} INFO - Started process (PID=1149) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:36:48.994+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:36:48.996+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:36:48.996+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:36:49.135+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:36:49.158+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:36:49.157+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:36:49.171+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:36:49.171+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:36:49.189+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T14:37:19.409+0000] {processor.py:186} INFO - Started process (PID=1156) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:37:19.410+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:37:19.412+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:37:19.411+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:37:19.554+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:37:19.577+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:37:19.577+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:37:19.591+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:37:19.591+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:37:19.615+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.211 seconds
[2025-01-10T14:37:49.842+0000] {processor.py:186} INFO - Started process (PID=1162) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:37:49.843+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:37:49.845+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:37:49.845+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:37:49.988+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:37:50.010+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:37:50.010+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:37:50.023+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:37:50.023+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:37:50.062+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.225 seconds
[2025-01-10T14:38:20.327+0000] {processor.py:186} INFO - Started process (PID=1169) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:38:20.328+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:38:20.330+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:38:20.330+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:38:20.469+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:38:20.490+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:38:20.490+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:38:20.503+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:38:20.503+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:38:20.522+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T14:38:50.866+0000] {processor.py:186} INFO - Started process (PID=1177) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:38:50.867+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:38:50.870+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:38:50.869+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:38:51.012+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:38:51.033+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:38:51.033+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:38:51.046+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:38:51.046+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:38:51.069+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.212 seconds
[2025-01-10T14:39:21.221+0000] {processor.py:186} INFO - Started process (PID=1185) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:39:21.221+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:39:21.224+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:39:21.223+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:39:21.364+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:39:21.386+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:39:21.386+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:39:21.401+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:39:21.400+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:39:21.417+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T14:39:51.625+0000] {processor.py:186} INFO - Started process (PID=1192) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:39:51.626+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:39:51.628+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:39:51.628+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:39:51.770+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:39:51.792+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:39:51.791+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:39:51.805+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:39:51.804+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:39:51.821+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T14:40:22.150+0000] {processor.py:186} INFO - Started process (PID=1199) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:40:22.151+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:40:22.153+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:40:22.153+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:40:22.306+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:40:22.333+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:40:22.333+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:40:22.349+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:40:22.349+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:40:22.372+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.234 seconds
[2025-01-10T14:40:52.515+0000] {processor.py:186} INFO - Started process (PID=1206) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:40:52.523+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:40:52.526+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:40:52.525+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:40:52.695+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:40:52.718+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:40:52.717+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:40:52.740+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:40:52.740+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:40:52.758+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.249 seconds
[2025-01-10T14:41:23.032+0000] {processor.py:186} INFO - Started process (PID=1213) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:41:23.033+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:41:23.035+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:41:23.035+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:41:23.177+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:41:23.199+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:41:23.198+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:41:23.213+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:41:23.213+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:41:23.243+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.216 seconds
[2025-01-10T14:41:53.297+0000] {processor.py:186} INFO - Started process (PID=1221) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:41:53.298+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:41:53.301+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:41:53.300+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:41:53.453+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:41:53.474+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:41:53.473+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:41:53.487+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:41:53.487+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:41:53.511+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.219 seconds
[2025-01-10T14:42:23.587+0000] {processor.py:186} INFO - Started process (PID=1228) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:42:23.588+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:42:23.590+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:42:23.590+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:42:23.730+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:42:23.752+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:42:23.752+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:42:23.765+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:42:23.765+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:42:23.788+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T14:42:53.920+0000] {processor.py:186} INFO - Started process (PID=1234) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:42:53.921+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:42:53.923+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:42:53.923+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:42:54.063+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:42:54.085+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:42:54.085+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:42:54.098+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:42:54.098+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:42:54.115+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T14:43:24.435+0000] {processor.py:186} INFO - Started process (PID=1241) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:43:24.436+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:43:24.438+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:43:24.438+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:43:24.579+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:43:24.601+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:43:24.600+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:43:24.615+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:43:24.614+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:43:24.638+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T14:43:54.691+0000] {processor.py:186} INFO - Started process (PID=1248) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:43:54.691+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:43:54.694+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:43:54.693+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:43:54.836+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:43:54.858+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:43:54.858+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:43:54.871+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:43:54.871+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:43:54.893+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T14:44:24.988+0000] {processor.py:186} INFO - Started process (PID=1255) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:44:24.989+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:44:24.991+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:44:24.991+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:44:25.131+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:44:25.152+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:44:25.152+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:44:25.165+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:44:25.165+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:44:25.187+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T14:44:55.307+0000] {processor.py:186} INFO - Started process (PID=1263) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:44:55.308+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:44:55.311+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:44:55.310+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:44:55.453+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:44:55.475+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:44:55.475+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:44:55.490+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:44:55.490+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:44:55.511+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T14:45:25.624+0000] {processor.py:186} INFO - Started process (PID=1270) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:45:25.625+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:45:25.627+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:45:25.627+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:45:25.765+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:45:25.787+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:45:25.787+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:45:25.801+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:45:25.801+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:45:25.823+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T14:45:55.972+0000] {processor.py:186} INFO - Started process (PID=1277) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:45:55.973+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:45:55.975+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:45:55.975+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:45:56.115+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:45:56.137+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:45:56.137+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:45:56.150+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:45:56.150+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:45:56.173+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T14:46:26.561+0000] {processor.py:186} INFO - Started process (PID=1284) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:46:26.562+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:46:26.565+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:46:26.564+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:46:26.705+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:46:26.727+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:46:26.726+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:46:26.739+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:46:26.739+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:46:26.762+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T14:46:56.950+0000] {processor.py:186} INFO - Started process (PID=1291) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:46:56.950+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:46:56.953+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:46:56.952+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:46:57.095+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:46:57.116+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:46:57.116+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:46:57.129+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:46:57.129+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:46:57.150+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T14:47:27.450+0000] {processor.py:186} INFO - Started process (PID=1298) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:47:27.451+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:47:27.453+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:47:27.453+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:47:27.592+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:47:27.613+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:47:27.613+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:47:27.627+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:47:27.627+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:47:27.643+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.198 seconds
[2025-01-10T14:47:57.844+0000] {processor.py:186} INFO - Started process (PID=1305) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:47:57.844+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:47:57.847+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:47:57.846+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:47:57.987+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:47:58.008+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:47:58.007+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:47:58.021+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:47:58.021+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:47:58.036+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.198 seconds
[2025-01-10T14:48:28.184+0000] {processor.py:186} INFO - Started process (PID=1313) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:48:28.185+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:48:28.187+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:48:28.186+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:48:28.325+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:48:28.347+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:48:28.347+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:48:28.360+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:48:28.360+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:48:28.377+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.199 seconds
[2025-01-10T14:48:58.762+0000] {processor.py:186} INFO - Started process (PID=1320) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:48:58.763+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:48:58.766+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:48:58.765+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:48:58.921+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:48:58.942+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:48:58.942+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:48:58.959+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:48:58.958+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:48:58.980+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.223 seconds
[2025-01-10T14:49:29.137+0000] {processor.py:186} INFO - Started process (PID=1328) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:49:29.137+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:49:29.140+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:49:29.140+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:49:29.291+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:49:29.315+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:49:29.315+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:49:29.329+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:49:29.329+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:49:29.351+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.221 seconds
[2025-01-10T14:49:59.700+0000] {processor.py:186} INFO - Started process (PID=1335) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:49:59.701+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:49:59.703+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:49:59.703+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:49:59.842+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:49:59.865+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:49:59.865+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:49:59.879+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:49:59.879+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:49:59.897+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T14:50:30.075+0000] {processor.py:186} INFO - Started process (PID=1342) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:50:30.076+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:50:30.078+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:50:30.077+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:50:30.216+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:50:30.237+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:50:30.237+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:50:30.252+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:50:30.252+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:50:30.275+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T14:51:00.613+0000] {processor.py:186} INFO - Started process (PID=1349) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:51:00.613+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:51:00.615+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:51:00.615+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:51:00.758+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:51:00.779+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:51:00.779+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:51:00.792+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:51:00.792+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:51:00.815+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T14:51:31.005+0000] {processor.py:186} INFO - Started process (PID=1356) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:51:31.005+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:51:31.008+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:51:31.007+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:51:31.149+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:51:31.171+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:51:31.170+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:51:31.184+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:51:31.184+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:51:31.200+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T14:52:01.486+0000] {processor.py:186} INFO - Started process (PID=1363) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:52:01.487+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:52:01.489+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:52:01.488+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:52:01.631+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:52:01.654+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:52:01.653+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:52:01.667+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:52:01.667+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:52:01.683+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T14:52:31.919+0000] {processor.py:186} INFO - Started process (PID=1369) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:52:31.920+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:52:31.923+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:52:31.922+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:52:32.065+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:52:32.086+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:52:32.086+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:52:32.099+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:52:32.099+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:52:32.115+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T14:53:02.366+0000] {processor.py:186} INFO - Started process (PID=1375) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:53:02.367+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:53:02.369+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:53:02.369+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:53:02.507+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:53:02.529+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:53:02.528+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:53:02.542+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:53:02.542+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:53:02.566+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T14:53:32.835+0000] {processor.py:186} INFO - Started process (PID=1382) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:53:32.835+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:53:32.838+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:53:32.837+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:53:32.996+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:53:33.018+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:53:33.018+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:53:33.032+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:53:33.031+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:53:33.054+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.225 seconds
[2025-01-10T14:54:03.203+0000] {processor.py:186} INFO - Started process (PID=1396) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:54:03.204+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:54:03.206+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:54:03.205+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:54:03.364+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:54:03.389+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:54:03.388+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:54:03.405+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:54:03.404+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:54:03.424+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.226 seconds
[2025-01-10T14:54:33.763+0000] {processor.py:186} INFO - Started process (PID=1404) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:54:33.764+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:54:33.766+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:54:33.766+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:54:33.901+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:54:33.924+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:54:33.923+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:54:33.936+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:54:33.936+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:54:33.960+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T14:55:04.116+0000] {processor.py:186} INFO - Started process (PID=1411) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:55:04.116+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:55:04.118+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:55:04.118+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:55:04.262+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:55:04.284+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:55:04.283+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:55:04.296+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:55:04.296+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:55:04.317+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T14:55:34.657+0000] {processor.py:186} INFO - Started process (PID=1419) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:55:34.658+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:55:34.660+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:55:34.660+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:55:34.800+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:55:34.821+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:55:34.821+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:55:34.834+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:55:34.834+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:55:34.851+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.199 seconds
[2025-01-10T14:56:05.037+0000] {processor.py:186} INFO - Started process (PID=1427) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:56:05.038+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:56:05.040+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:56:05.040+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:56:05.179+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:56:05.201+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:56:05.200+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:56:05.213+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:56:05.213+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:56:05.230+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.199 seconds
[2025-01-10T14:56:35.433+0000] {processor.py:186} INFO - Started process (PID=1435) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:56:35.448+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:56:35.450+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:56:35.450+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:56:35.788+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:56:35.818+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:56:35.818+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:56:35.848+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:56:35.847+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:56:35.889+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.461 seconds
[2025-01-10T14:57:06.061+0000] {processor.py:186} INFO - Started process (PID=1442) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:57:06.062+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:57:06.065+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:57:06.064+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:57:06.238+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:57:06.260+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:57:06.260+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:57:06.274+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:57:06.274+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:57:06.297+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.241 seconds
[2025-01-10T14:57:36.647+0000] {processor.py:186} INFO - Started process (PID=1449) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:57:36.648+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:57:36.650+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:57:36.650+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:57:36.789+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:57:36.810+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:57:36.810+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:57:36.824+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:57:36.824+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:57:36.846+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T14:58:07.005+0000] {processor.py:186} INFO - Started process (PID=1457) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:58:07.006+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:58:07.008+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:58:07.008+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:58:07.151+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:58:07.174+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:58:07.174+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:58:07.187+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:58:07.187+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:58:07.203+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T14:58:37.555+0000] {processor.py:186} INFO - Started process (PID=1464) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:58:37.556+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:58:37.558+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:58:37.558+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:58:37.709+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:58:37.730+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:58:37.729+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:58:37.743+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:58:37.743+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:58:37.759+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T14:59:07.911+0000] {processor.py:186} INFO - Started process (PID=1472) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:59:07.912+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:59:07.914+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:59:07.914+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:59:08.058+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:59:08.080+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:59:08.080+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:59:08.094+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:59:08.094+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:59:08.110+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T14:59:38.425+0000] {processor.py:186} INFO - Started process (PID=1480) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T14:59:38.426+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T14:59:38.428+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:59:38.428+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:59:38.570+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T14:59:38.591+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:59:38.590+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T14:59:38.604+0000] {logging_mixin.py:190} INFO - [2025-01-10T14:59:38.604+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T14:59:38.621+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T15:00:08.815+0000] {processor.py:186} INFO - Started process (PID=1487) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:00:08.816+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:00:08.818+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:00:08.818+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:00:08.963+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:00:08.985+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:00:08.984+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:00:08.998+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:00:08.997+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:00:09.016+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T15:00:39.282+0000] {processor.py:186} INFO - Started process (PID=1495) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:00:39.283+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:00:39.285+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:00:39.285+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:00:39.424+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:00:39.445+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:00:39.445+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:00:39.458+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:00:39.457+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:00:39.487+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.210 seconds
[2025-01-10T15:01:09.642+0000] {processor.py:186} INFO - Started process (PID=1502) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:01:09.643+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:01:09.645+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:01:09.645+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:01:09.786+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:01:09.808+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:01:09.808+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:01:09.822+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:01:09.821+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:01:09.849+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.212 seconds
[2025-01-10T15:01:40.071+0000] {processor.py:186} INFO - Started process (PID=1509) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:01:40.072+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:01:40.074+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:01:40.074+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:01:40.214+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:01:40.235+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:01:40.235+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:01:40.249+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:01:40.249+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:01:40.265+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T15:02:10.496+0000] {processor.py:186} INFO - Started process (PID=1516) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:02:10.497+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:02:10.499+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:02:10.499+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:02:10.648+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:02:10.670+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:02:10.669+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:02:10.683+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:02:10.683+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:02:10.698+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T15:02:40.961+0000] {processor.py:186} INFO - Started process (PID=1523) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:02:40.962+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:02:40.964+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:02:40.964+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:02:41.104+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:02:41.125+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:02:41.125+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:02:41.138+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:02:41.138+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:02:41.159+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T15:03:11.324+0000] {processor.py:186} INFO - Started process (PID=1530) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:03:11.325+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:03:11.327+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:03:11.326+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:03:11.466+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:03:11.487+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:03:11.487+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:03:11.500+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:03:11.500+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:03:11.516+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.198 seconds
[2025-01-10T15:03:41.821+0000] {processor.py:186} INFO - Started process (PID=1537) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:03:41.822+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:03:41.825+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:03:41.824+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:03:41.965+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:03:41.987+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:03:41.986+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:03:42.000+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:03:42.000+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:03:42.017+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T15:04:12.148+0000] {processor.py:186} INFO - Started process (PID=1545) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:04:12.149+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:04:12.152+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:04:12.151+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:04:12.336+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:04:12.362+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:04:12.362+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:04:12.379+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:04:12.379+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:04:12.397+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.255 seconds
[2025-01-10T15:04:42.619+0000] {processor.py:186} INFO - Started process (PID=1553) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:04:42.620+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:04:42.622+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:04:42.622+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:04:42.768+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:04:42.791+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:04:42.791+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:04:42.804+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:04:42.804+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:04:42.824+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.218 seconds
[2025-01-10T15:05:13.108+0000] {processor.py:186} INFO - Started process (PID=1560) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:05:13.109+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:05:13.112+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:05:13.112+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:05:13.301+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:05:13.321+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:05:13.320+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:05:13.333+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:05:13.333+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:05:13.354+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.252 seconds
[2025-01-10T15:05:43.544+0000] {processor.py:186} INFO - Started process (PID=1567) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:05:43.545+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:05:43.547+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:05:43.547+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:05:43.687+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:05:43.708+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:05:43.708+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:05:43.722+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:05:43.722+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:05:43.745+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T15:06:14.051+0000] {processor.py:186} INFO - Started process (PID=1574) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:06:14.052+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:06:14.055+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:06:14.054+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:06:14.203+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:06:14.225+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:06:14.225+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:06:14.239+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:06:14.239+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:06:14.257+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.211 seconds
[2025-01-10T15:06:44.517+0000] {processor.py:186} INFO - Started process (PID=1581) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:06:44.518+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:06:44.520+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:06:44.520+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:06:44.662+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:06:44.685+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:06:44.684+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:06:44.698+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:06:44.697+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:06:44.720+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T15:07:15.037+0000] {processor.py:186} INFO - Started process (PID=1589) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:07:15.038+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:07:15.041+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:07:15.040+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:07:15.189+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:07:15.210+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:07:15.209+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:07:15.223+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:07:15.222+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:07:15.271+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.238 seconds
[2025-01-10T15:07:45.361+0000] {processor.py:186} INFO - Started process (PID=1597) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:07:45.361+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:07:45.364+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:07:45.363+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:07:45.503+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:07:45.526+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:07:45.526+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:07:45.541+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:07:45.540+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:07:45.557+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T15:08:15.705+0000] {processor.py:186} INFO - Started process (PID=1605) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:08:15.706+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:08:15.708+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:08:15.707+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:08:15.850+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:08:15.873+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:08:15.873+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:08:15.886+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:08:15.885+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:08:15.902+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T15:08:46.032+0000] {processor.py:186} INFO - Started process (PID=1612) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:08:46.033+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:08:46.036+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:08:46.035+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:08:46.177+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:08:46.199+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:08:46.199+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:08:46.212+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:08:46.211+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:08:46.227+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T15:09:16.575+0000] {processor.py:186} INFO - Started process (PID=1620) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:09:16.576+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:09:16.578+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:09:16.577+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:09:16.728+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:09:16.749+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:09:16.749+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:09:16.762+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:09:16.762+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:09:16.780+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.211 seconds
[2025-01-10T15:09:46.950+0000] {processor.py:186} INFO - Started process (PID=1628) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:09:46.951+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:09:46.953+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:09:46.953+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:09:47.093+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:09:47.115+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:09:47.114+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:09:47.128+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:09:47.128+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:09:47.266+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.321 seconds
[2025-01-10T15:10:17.443+0000] {processor.py:186} INFO - Started process (PID=1636) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:10:17.443+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:10:17.446+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:10:17.445+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:10:17.593+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:10:17.616+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:10:17.615+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:10:17.629+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:10:17.629+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:10:17.647+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.210 seconds
[2025-01-10T15:10:47.998+0000] {processor.py:186} INFO - Started process (PID=1643) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:10:47.999+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:10:48.001+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:10:48.001+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:10:48.140+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:10:48.162+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:10:48.162+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:10:48.175+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:10:48.175+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:10:48.216+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.223 seconds
[2025-01-10T15:11:18.322+0000] {processor.py:186} INFO - Started process (PID=1650) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:11:18.323+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:11:18.325+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:11:18.324+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:11:18.464+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:11:18.485+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:11:18.484+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:11:18.498+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:11:18.498+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:11:18.528+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.211 seconds
[2025-01-10T15:11:48.834+0000] {processor.py:186} INFO - Started process (PID=1658) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:11:48.837+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:11:48.839+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:11:48.839+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:11:49.001+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:11:49.023+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:11:49.022+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:11:49.038+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:11:49.037+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:11:49.181+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.352 seconds
[2025-01-10T15:12:19.534+0000] {processor.py:186} INFO - Started process (PID=1665) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:12:19.535+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:12:19.537+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:12:19.537+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:12:19.687+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:12:19.712+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:12:19.711+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:12:19.726+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:12:19.726+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:12:19.742+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.215 seconds
[2025-01-10T15:12:49.878+0000] {processor.py:186} INFO - Started process (PID=1672) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:12:49.879+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:12:49.882+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:12:49.881+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:12:50.022+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:12:50.044+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:12:50.044+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:12:50.057+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:12:50.056+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:12:50.079+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T15:13:20.388+0000] {processor.py:186} INFO - Started process (PID=1680) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:13:20.389+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:13:20.391+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:13:20.391+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:13:20.536+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:13:20.557+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:13:20.556+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:13:20.570+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:13:20.569+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:13:20.585+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T15:13:50.751+0000] {processor.py:186} INFO - Started process (PID=1687) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:13:50.752+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:13:50.755+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:13:50.754+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:13:50.914+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:13:50.941+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:13:50.941+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:13:50.957+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:13:50.956+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:13:51.139+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.393 seconds
[2025-01-10T15:14:21.243+0000] {processor.py:186} INFO - Started process (PID=1694) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:14:21.244+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:14:21.246+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:14:21.246+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:14:21.387+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:14:21.410+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:14:21.410+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:14:21.423+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:14:21.423+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:14:21.440+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T15:14:51.780+0000] {processor.py:186} INFO - Started process (PID=1702) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:14:51.781+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:14:51.783+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:14:51.783+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:14:51.923+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:14:51.944+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:14:51.943+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:14:51.957+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:14:51.956+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:14:51.979+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T15:15:22.086+0000] {processor.py:186} INFO - Started process (PID=1709) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:15:22.086+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:15:22.089+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:15:22.088+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:15:22.229+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:15:22.252+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:15:22.252+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:15:22.265+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:15:22.265+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:15:22.283+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T15:15:52.670+0000] {processor.py:186} INFO - Started process (PID=1716) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:15:52.671+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:15:52.673+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:15:52.673+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:15:52.812+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:15:52.833+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:15:52.833+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:15:52.847+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:15:52.847+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:15:52.974+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.309 seconds
[2025-01-10T15:16:23.104+0000] {processor.py:186} INFO - Started process (PID=1723) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:16:23.105+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:16:23.107+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:16:23.107+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:16:23.247+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:16:23.269+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:16:23.269+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:16:23.284+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:16:23.283+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:16:23.299+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T15:16:53.606+0000] {processor.py:186} INFO - Started process (PID=1730) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:16:53.606+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:16:53.609+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:16:53.608+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:16:53.749+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:16:53.771+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:16:53.771+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:16:53.785+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:16:53.785+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:16:53.802+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T15:17:23.939+0000] {processor.py:186} INFO - Started process (PID=1738) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:17:23.940+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:17:23.942+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:17:23.942+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:17:24.085+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:17:24.107+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:17:24.106+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:17:24.119+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:17:24.119+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:17:24.143+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T15:17:54.480+0000] {processor.py:186} INFO - Started process (PID=1745) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:17:54.481+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:17:54.483+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:17:54.483+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:17:54.626+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:17:54.647+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:17:54.647+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:17:54.661+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:17:54.661+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:17:54.787+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.315 seconds
[2025-01-10T15:18:25.176+0000] {processor.py:186} INFO - Started process (PID=1752) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:18:25.177+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:18:25.179+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:18:25.179+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:18:25.321+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:18:25.344+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:18:25.343+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:18:25.357+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:18:25.356+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:18:25.393+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.221 seconds
[2025-01-10T15:18:55.532+0000] {processor.py:186} INFO - Started process (PID=1760) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:18:55.533+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:18:55.535+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:18:55.535+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:18:55.676+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:18:55.698+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:18:55.697+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:18:55.711+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:18:55.711+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:18:55.728+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T15:19:26.082+0000] {processor.py:186} INFO - Started process (PID=1767) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:19:26.083+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:19:26.085+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:19:26.085+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:19:26.332+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:19:26.350+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:19:26.349+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:19:26.361+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:19:26.360+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:19:26.382+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.305 seconds
[2025-01-10T15:19:56.440+0000] {processor.py:186} INFO - Started process (PID=1774) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:19:56.441+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:19:56.443+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:19:56.443+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:19:56.720+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:19:56.741+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:19:56.741+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:19:56.755+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:19:56.755+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:19:56.770+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.336 seconds
[2025-01-10T15:20:27.066+0000] {processor.py:186} INFO - Started process (PID=1782) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:20:27.067+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:20:27.069+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:20:27.068+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:20:27.222+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:20:27.371+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:20:27.371+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:20:27.382+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:20:27.382+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:20:27.397+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.337 seconds
[2025-01-10T15:20:57.790+0000] {processor.py:186} INFO - Started process (PID=1790) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:20:57.790+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:20:57.793+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:20:57.792+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:20:57.933+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:20:57.956+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:20:57.956+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:20:57.969+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:20:57.969+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:20:58.010+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.226 seconds
[2025-01-10T15:21:28.183+0000] {processor.py:186} INFO - Started process (PID=1797) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:21:28.183+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:21:28.186+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:21:28.185+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:21:28.447+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:21:28.466+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:21:28.465+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:21:28.476+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:21:28.476+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:21:28.492+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.315 seconds
[2025-01-10T15:21:58.807+0000] {processor.py:186} INFO - Started process (PID=1804) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:21:58.808+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:21:58.810+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:21:58.810+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:21:59.064+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:21:59.082+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:21:59.082+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:21:59.098+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:21:59.098+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:21:59.114+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.312 seconds
[2025-01-10T15:22:29.465+0000] {processor.py:186} INFO - Started process (PID=1811) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:22:29.466+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:22:29.469+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:22:29.468+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:22:29.616+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:22:29.758+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:22:29.758+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:22:29.769+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:22:29.769+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:22:29.802+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.343 seconds
[2025-01-10T15:22:59.875+0000] {processor.py:186} INFO - Started process (PID=1818) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:22:59.876+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:22:59.878+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:22:59.878+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:23:00.035+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:23:00.060+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:23:00.059+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:23:00.075+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:23:00.075+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:23:00.093+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.223 seconds
[2025-01-10T15:23:30.189+0000] {processor.py:186} INFO - Started process (PID=1825) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:23:30.190+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:23:30.192+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:23:30.192+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:23:30.440+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:23:30.458+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:23:30.458+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:23:30.469+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:23:30.469+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:23:30.484+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.301 seconds
[2025-01-10T15:24:00.812+0000] {processor.py:186} INFO - Started process (PID=1833) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:24:00.813+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:24:00.815+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:24:00.815+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:24:01.061+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:24:01.080+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:24:01.079+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:24:01.093+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:24:01.093+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:24:01.109+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.302 seconds
[2025-01-10T15:24:31.258+0000] {processor.py:186} INFO - Started process (PID=1840) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:24:31.259+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:24:31.262+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:24:31.261+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:24:31.402+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:24:31.536+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:24:31.535+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:24:31.547+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:24:31.546+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:24:31.567+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.314 seconds
[2025-01-10T15:25:01.760+0000] {processor.py:186} INFO - Started process (PID=1847) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:25:01.760+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:25:01.762+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:25:01.762+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:25:01.900+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:25:01.922+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:25:01.921+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:25:01.936+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:25:01.936+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:25:01.960+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T15:25:32.204+0000] {processor.py:186} INFO - Started process (PID=1855) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:25:32.205+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:25:32.208+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:25:32.207+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:25:32.456+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:25:32.473+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:25:32.473+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:25:32.484+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:25:32.484+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:25:32.499+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.300 seconds
[2025-01-10T15:26:02.556+0000] {processor.py:186} INFO - Started process (PID=1861) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:26:02.556+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:26:02.559+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:26:02.558+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:26:02.816+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:26:02.835+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:26:02.835+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:26:02.847+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:26:02.847+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:26:02.862+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.312 seconds
[2025-01-10T15:26:33.178+0000] {processor.py:186} INFO - Started process (PID=1869) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:26:33.179+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:26:33.181+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:26:33.181+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:26:33.429+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:26:33.448+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:26:33.447+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:26:33.459+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:26:33.459+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:26:33.475+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.303 seconds
[2025-01-10T15:27:03.528+0000] {processor.py:186} INFO - Started process (PID=1876) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:27:03.529+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:27:03.531+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:27:03.531+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:27:03.788+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:27:03.806+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:27:03.806+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:27:03.817+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:27:03.816+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:27:03.837+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.315 seconds
[2025-01-10T15:27:34.186+0000] {processor.py:186} INFO - Started process (PID=1883) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:27:34.187+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:27:34.189+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:27:34.189+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:27:34.443+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:27:34.462+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:27:34.461+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:27:34.472+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:27:34.472+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:27:34.494+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.314 seconds
[2025-01-10T15:28:04.586+0000] {processor.py:186} INFO - Started process (PID=1891) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:28:04.587+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:28:04.589+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:28:04.589+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:28:04.867+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:28:04.886+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:28:04.885+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:28:04.899+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:28:04.899+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:28:04.917+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.338 seconds
[2025-01-10T15:28:35.189+0000] {processor.py:186} INFO - Started process (PID=1904) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:28:35.190+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:28:35.192+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:28:35.192+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:28:35.438+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:28:35.456+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:28:35.456+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:28:35.467+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:28:35.467+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:28:35.487+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.303 seconds
[2025-01-10T15:29:05.918+0000] {processor.py:186} INFO - Started process (PID=1912) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:29:05.919+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:29:05.921+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:29:05.921+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:29:06.175+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:29:06.194+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:29:06.194+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:29:06.205+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:29:06.205+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:29:06.220+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.308 seconds
[2025-01-10T15:29:36.558+0000] {processor.py:186} INFO - Started process (PID=1918) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:29:36.559+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:29:36.562+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:29:36.561+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:29:36.811+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:29:36.829+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:29:36.829+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:29:36.840+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:29:36.840+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:29:36.855+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.302 seconds
[2025-01-10T15:30:06.933+0000] {processor.py:186} INFO - Started process (PID=1925) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:30:06.934+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:30:06.936+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:30:06.936+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:30:07.190+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:30:07.208+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:30:07.208+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:30:07.220+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:30:07.219+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:30:07.235+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.308 seconds
[2025-01-10T15:30:37.472+0000] {processor.py:186} INFO - Started process (PID=1933) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:30:37.473+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:30:37.475+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:30:37.475+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:30:37.724+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:30:37.744+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:30:37.743+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:30:37.754+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:30:37.754+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:30:37.774+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.308 seconds
[2025-01-10T15:31:08.007+0000] {processor.py:186} INFO - Started process (PID=1940) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:31:08.008+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:31:08.010+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:31:08.010+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:31:08.264+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:31:08.283+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:31:08.283+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:31:08.294+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:31:08.294+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:31:08.309+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.307 seconds
[2025-01-10T15:31:38.501+0000] {processor.py:186} INFO - Started process (PID=1947) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:31:38.502+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:31:38.504+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:31:38.504+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:31:38.807+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:31:38.841+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:31:38.841+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:31:38.860+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:31:38.860+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:31:38.881+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.386 seconds
[2025-01-10T15:32:08.980+0000] {processor.py:186} INFO - Started process (PID=1954) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:32:08.981+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:32:08.983+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:32:08.983+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:32:09.236+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:32:09.254+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:32:09.254+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:32:09.266+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:32:09.266+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:32:09.282+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.307 seconds
[2025-01-10T15:32:39.712+0000] {processor.py:186} INFO - Started process (PID=1961) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:32:39.713+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:32:39.715+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:32:39.715+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:32:39.857+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:32:39.879+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:32:39.879+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:32:39.892+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:32:39.892+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:32:39.915+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T15:33:10.072+0000] {processor.py:186} INFO - Started process (PID=1968) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:33:10.073+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:33:10.076+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:33:10.075+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:33:10.216+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:33:10.241+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:33:10.240+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:33:10.254+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:33:10.253+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:33:10.270+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T15:33:40.662+0000] {processor.py:186} INFO - Started process (PID=1975) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:33:40.663+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:33:40.665+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:33:40.665+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:33:40.804+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:33:40.825+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:33:40.824+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:33:40.837+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:33:40.837+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:33:40.853+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.197 seconds
[2025-01-10T15:34:11.036+0000] {processor.py:186} INFO - Started process (PID=1983) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:34:11.037+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:34:11.039+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:34:11.039+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:34:11.189+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:34:11.211+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:34:11.211+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:34:11.226+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:34:11.225+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:34:11.243+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.213 seconds
[2025-01-10T15:34:41.555+0000] {processor.py:186} INFO - Started process (PID=1990) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:34:41.556+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:34:41.558+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:34:41.558+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:34:41.708+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:34:41.731+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:34:41.730+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:34:41.744+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:34:41.744+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:34:41.766+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.217 seconds
[2025-01-10T15:35:11.991+0000] {processor.py:186} INFO - Started process (PID=1998) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:35:11.992+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:35:11.994+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:35:11.994+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:35:12.137+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:35:12.160+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:35:12.160+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:35:12.173+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:35:12.173+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:35:12.195+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.211 seconds
[2025-01-10T15:35:42.482+0000] {processor.py:186} INFO - Started process (PID=2005) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:35:42.483+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:35:42.486+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:35:42.485+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:35:42.632+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:35:42.654+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:35:42.654+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:35:42.667+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:35:42.667+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:35:42.691+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.214 seconds
[2025-01-10T15:36:12.946+0000] {processor.py:186} INFO - Started process (PID=2012) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:36:12.947+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:36:12.949+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:36:12.949+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:36:13.111+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:36:13.133+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:36:13.132+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:36:13.147+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:36:13.147+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:36:13.163+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.223 seconds
[2025-01-10T15:36:43.422+0000] {processor.py:186} INFO - Started process (PID=2019) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:36:43.423+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:36:43.425+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:36:43.425+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:36:43.572+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:36:43.595+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:36:43.595+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:36:43.608+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:36:43.608+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:36:43.635+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.219 seconds
[2025-01-10T15:37:13.742+0000] {processor.py:186} INFO - Started process (PID=2025) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:37:13.743+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:37:13.746+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:37:13.746+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:37:13.888+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:37:13.909+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:37:13.909+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:37:13.922+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:37:13.922+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:37:13.938+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T15:37:44.043+0000] {processor.py:186} INFO - Started process (PID=2033) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:37:44.044+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:37:44.046+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:37:44.046+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:37:44.187+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:37:44.209+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:37:44.209+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:37:44.222+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:37:44.222+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:37:44.238+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T15:38:14.330+0000] {processor.py:186} INFO - Started process (PID=2040) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:38:14.330+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:38:14.332+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:38:14.332+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:38:14.471+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:38:14.493+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:38:14.492+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:38:14.507+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:38:14.506+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:38:14.553+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.228 seconds
[2025-01-10T15:38:44.671+0000] {processor.py:186} INFO - Started process (PID=2047) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:38:44.672+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:38:44.675+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:38:44.674+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:38:44.813+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:38:44.835+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:38:44.835+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:38:44.848+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:38:44.848+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:38:44.865+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.199 seconds
[2025-01-10T15:39:15.030+0000] {processor.py:186} INFO - Started process (PID=2054) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:39:15.031+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:39:15.033+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:39:15.033+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:39:15.172+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:39:15.195+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:39:15.194+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:39:15.209+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:39:15.209+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:39:15.226+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T15:39:45.491+0000] {processor.py:186} INFO - Started process (PID=2061) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:39:45.491+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:39:45.494+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:39:45.494+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:39:45.644+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:39:45.666+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:39:45.666+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:39:45.680+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:39:45.680+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:39:45.713+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.229 seconds
[2025-01-10T15:40:15.918+0000] {processor.py:186} INFO - Started process (PID=2068) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:40:15.919+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:40:15.921+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:40:15.921+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:40:16.095+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:40:16.122+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:40:16.122+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:40:16.136+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:40:16.136+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:40:16.157+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.244 seconds
[2025-01-10T15:40:46.467+0000] {processor.py:186} INFO - Started process (PID=2074) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:40:46.468+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:40:46.470+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:40:46.470+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:40:46.613+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:40:46.635+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:40:46.634+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:40:46.647+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:40:46.647+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:40:46.670+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T15:41:16.937+0000] {processor.py:186} INFO - Started process (PID=2081) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:41:16.938+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:41:16.941+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:41:16.940+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:41:17.084+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:41:17.105+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:41:17.104+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:41:17.118+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:41:17.117+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:41:17.134+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T15:41:47.410+0000] {processor.py:186} INFO - Started process (PID=2088) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:41:47.411+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:41:47.413+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:41:47.412+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:41:47.555+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:41:47.578+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:41:47.577+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:41:47.591+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:41:47.590+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:41:47.608+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T15:42:17.820+0000] {processor.py:186} INFO - Started process (PID=2095) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:42:17.820+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:42:17.823+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:42:17.822+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:42:17.963+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:42:17.983+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:42:17.983+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:42:17.996+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:42:17.996+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:42:18.014+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T15:42:48.311+0000] {processor.py:186} INFO - Started process (PID=2103) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:42:48.312+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:42:48.314+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:42:48.314+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:42:48.454+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:42:48.477+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:42:48.476+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:42:48.490+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:42:48.489+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:42:48.512+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T15:43:18.641+0000] {processor.py:186} INFO - Started process (PID=2111) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:43:18.642+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:43:18.644+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:43:18.644+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:43:18.786+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:43:18.810+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:43:18.809+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:43:18.822+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:43:18.822+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:43:18.839+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T15:43:48.960+0000] {processor.py:186} INFO - Started process (PID=2118) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:43:48.961+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:43:48.963+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:43:48.963+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:43:49.110+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:43:49.134+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:43:49.133+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:43:49.155+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:43:49.154+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:43:49.180+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.226 seconds
[2025-01-10T15:44:19.592+0000] {processor.py:186} INFO - Started process (PID=2125) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:44:19.593+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:44:19.600+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:44:19.600+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:44:19.755+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:44:19.777+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:44:19.776+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:44:19.791+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:44:19.790+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:44:19.814+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.228 seconds
[2025-01-10T15:44:49.981+0000] {processor.py:186} INFO - Started process (PID=2132) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:44:49.982+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:44:49.984+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:44:49.984+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:44:50.126+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:44:50.148+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:44:50.148+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:44:50.161+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:44:50.161+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:44:50.184+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T15:45:20.474+0000] {processor.py:186} INFO - Started process (PID=2139) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:45:20.475+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:45:20.478+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:45:20.477+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:45:20.620+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:45:20.641+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:45:20.640+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:45:20.654+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:45:20.654+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:45:20.671+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T15:45:51.004+0000] {processor.py:186} INFO - Started process (PID=2146) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:45:51.005+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:45:51.008+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:45:51.007+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:45:51.167+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:45:51.188+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:45:51.187+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:45:51.200+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:45:51.200+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:45:51.217+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.221 seconds
[2025-01-10T15:46:21.372+0000] {processor.py:186} INFO - Started process (PID=2153) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:46:21.373+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:46:21.375+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:46:21.375+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:46:21.515+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:46:21.537+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:46:21.536+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:46:21.550+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:46:21.549+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:46:21.568+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T15:46:51.638+0000] {processor.py:186} INFO - Started process (PID=2160) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:46:51.638+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:46:51.641+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:46:51.640+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:46:51.780+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:46:51.804+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:46:51.803+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:46:51.816+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:46:51.816+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:46:51.838+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T15:47:21.990+0000] {processor.py:186} INFO - Started process (PID=2168) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:47:21.991+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:47:21.993+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:47:21.992+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:47:22.129+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:47:22.151+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:47:22.151+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:47:22.164+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:47:22.164+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:47:22.187+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T15:47:52.341+0000] {processor.py:186} INFO - Started process (PID=2175) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:47:52.342+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:47:52.344+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:47:52.344+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:47:52.489+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:47:52.512+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:47:52.511+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:47:52.525+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:47:52.525+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:47:52.548+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.213 seconds
[2025-01-10T15:48:22.804+0000] {processor.py:186} INFO - Started process (PID=2181) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:48:22.805+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:48:22.807+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:48:22.807+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:48:22.951+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:48:22.972+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:48:22.972+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:48:22.985+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:48:22.985+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:48:23.023+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.225 seconds
[2025-01-10T15:48:53.225+0000] {processor.py:186} INFO - Started process (PID=2188) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:48:53.226+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:48:53.228+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:48:53.228+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:48:53.385+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:48:53.407+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:48:53.407+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:48:53.421+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:48:53.421+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:48:53.444+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.225 seconds
[2025-01-10T15:49:23.701+0000] {processor.py:186} INFO - Started process (PID=2195) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:49:23.702+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:49:23.704+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:49:23.704+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:49:23.848+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:49:23.870+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:49:23.870+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:49:23.883+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:49:23.883+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:49:23.900+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T15:49:54.183+0000] {processor.py:186} INFO - Started process (PID=2202) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:49:54.184+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:49:54.186+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:49:54.186+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:49:54.324+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:49:54.347+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:49:54.346+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:49:54.359+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:49:54.359+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:49:54.381+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T15:50:24.516+0000] {processor.py:186} INFO - Started process (PID=2209) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:50:24.517+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:50:24.520+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:50:24.519+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:50:24.672+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:50:24.694+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:50:24.694+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:50:24.708+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:50:24.707+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:50:24.728+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.218 seconds
[2025-01-10T15:50:55.078+0000] {processor.py:186} INFO - Started process (PID=2216) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:50:55.079+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:50:55.081+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:50:55.081+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:50:55.220+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:50:55.241+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:50:55.241+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:50:55.254+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:50:55.254+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:50:55.281+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T15:51:25.478+0000] {processor.py:186} INFO - Started process (PID=2224) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:51:25.479+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:51:25.481+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:51:25.481+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:51:25.621+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:51:25.641+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:51:25.641+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:51:25.654+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:51:25.654+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:51:25.676+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T15:51:56.009+0000] {processor.py:186} INFO - Started process (PID=2231) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:51:56.010+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:51:56.013+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:51:56.012+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:51:56.179+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:51:56.202+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:51:56.201+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:51:56.216+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:51:56.216+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:51:56.238+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.236 seconds
[2025-01-10T15:52:26.432+0000] {processor.py:186} INFO - Started process (PID=2238) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:52:26.433+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:52:26.435+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:52:26.434+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:52:26.576+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:52:26.597+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:52:26.596+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:52:26.609+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:52:26.609+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:52:26.626+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.199 seconds
[2025-01-10T15:52:56.945+0000] {processor.py:186} INFO - Started process (PID=2245) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:52:56.946+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:52:56.948+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:52:56.948+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:52:57.090+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:52:57.112+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:52:57.111+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:52:57.125+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:52:57.125+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:52:57.165+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.225 seconds
[2025-01-10T15:53:27.270+0000] {processor.py:186} INFO - Started process (PID=2252) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:53:27.270+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:53:27.273+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:53:27.273+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:53:27.415+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:53:27.439+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:53:27.438+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:53:27.452+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:53:27.451+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:53:27.469+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T15:53:57.570+0000] {processor.py:186} INFO - Started process (PID=2260) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:53:57.571+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:53:57.574+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:53:57.573+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:53:57.716+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:53:57.739+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:53:57.739+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:53:57.752+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:53:57.752+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:53:57.776+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.211 seconds
[2025-01-10T15:54:27.858+0000] {processor.py:186} INFO - Started process (PID=2267) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:54:27.858+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:54:27.861+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:54:27.861+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:54:28.003+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:54:28.026+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:54:28.025+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:54:28.039+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:54:28.038+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:54:28.064+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.212 seconds
[2025-01-10T15:54:58.209+0000] {processor.py:186} INFO - Started process (PID=2274) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:54:58.210+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:54:58.213+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:54:58.212+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:54:58.356+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:54:58.379+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:54:58.378+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:54:58.392+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:54:58.392+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:54:58.409+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T15:55:28.716+0000] {processor.py:186} INFO - Started process (PID=2281) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:55:28.717+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:55:28.719+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:55:28.719+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:55:28.857+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:55:28.878+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:55:28.878+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:55:28.891+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:55:28.891+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:55:28.907+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.197 seconds
[2025-01-10T15:55:59.048+0000] {processor.py:186} INFO - Started process (PID=2289) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:55:59.049+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:55:59.051+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:55:59.051+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:55:59.208+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:55:59.231+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:55:59.230+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:55:59.244+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:55:59.244+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:55:59.261+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.219 seconds
[2025-01-10T15:56:29.314+0000] {processor.py:186} INFO - Started process (PID=2297) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:56:29.315+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:56:29.317+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:56:29.317+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:56:29.459+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:56:29.481+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:56:29.480+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:56:29.494+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:56:29.493+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:56:29.511+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T15:56:59.607+0000] {processor.py:186} INFO - Started process (PID=2304) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:56:59.608+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:56:59.611+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:56:59.610+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:56:59.792+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:56:59.817+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:56:59.817+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:56:59.832+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:56:59.832+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:56:59.852+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.251 seconds
[2025-01-10T15:57:30.197+0000] {processor.py:186} INFO - Started process (PID=2312) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:57:30.198+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:57:30.200+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:57:30.200+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:57:30.340+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:57:30.374+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:57:30.372+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:57:30.391+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:57:30.391+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:57:30.413+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.220 seconds
[2025-01-10T15:58:00.567+0000] {processor.py:186} INFO - Started process (PID=2319) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:58:00.568+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:58:00.570+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:58:00.570+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:58:00.717+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:58:00.739+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:58:00.738+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:58:00.752+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:58:00.751+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:58:00.767+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T15:58:30.940+0000] {processor.py:186} INFO - Started process (PID=2326) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:58:30.940+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:58:30.943+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:58:30.942+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:58:31.083+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:58:31.105+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:58:31.105+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:58:31.118+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:58:31.118+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:58:31.142+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T15:59:01.411+0000] {processor.py:186} INFO - Started process (PID=2333) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:59:01.412+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:59:01.415+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:59:01.414+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:59:01.558+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:59:01.581+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:59:01.581+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:59:01.594+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:59:01.594+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:59:01.612+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T15:59:31.822+0000] {processor.py:186} INFO - Started process (PID=2340) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T15:59:31.823+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T15:59:31.826+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:59:31.825+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:59:31.980+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T15:59:32.003+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:59:32.003+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T15:59:32.018+0000] {logging_mixin.py:190} INFO - [2025-01-10T15:59:32.018+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T15:59:32.037+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.221 seconds
[2025-01-10T16:00:02.350+0000] {processor.py:186} INFO - Started process (PID=2347) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:00:02.351+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:00:02.353+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:00:02.352+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:00:02.494+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:00:02.516+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:00:02.515+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:00:02.528+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:00:02.528+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:00:02.560+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.215 seconds
[2025-01-10T16:00:32.715+0000] {processor.py:186} INFO - Started process (PID=2354) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:00:32.715+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:00:32.717+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:00:32.717+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:00:32.858+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:00:32.880+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:00:32.880+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:00:32.893+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:00:32.893+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:00:32.917+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T16:01:03.196+0000] {processor.py:186} INFO - Started process (PID=2362) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:01:03.196+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:01:03.198+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:01:03.198+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:01:03.339+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:01:03.361+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:01:03.360+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:01:03.374+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:01:03.373+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:01:03.395+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T16:01:33.487+0000] {processor.py:186} INFO - Started process (PID=2369) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:01:33.488+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:01:33.494+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:01:33.492+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:01:33.645+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:01:33.668+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:01:33.667+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:01:33.681+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:01:33.681+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:01:33.704+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.225 seconds
[2025-01-10T16:02:03.795+0000] {processor.py:186} INFO - Started process (PID=2377) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:02:03.796+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:02:03.798+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:02:03.798+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:02:03.945+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:02:03.967+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:02:03.967+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:02:03.980+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:02:03.980+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:02:04.002+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.212 seconds
[2025-01-10T16:02:34.065+0000] {processor.py:186} INFO - Started process (PID=2384) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:02:34.066+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:02:34.069+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:02:34.068+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:02:34.206+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:02:34.228+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:02:34.228+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:02:34.241+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:02:34.240+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:02:34.264+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T16:03:04.620+0000] {processor.py:186} INFO - Started process (PID=2391) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:03:04.621+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:03:04.623+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:03:04.623+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:03:04.769+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:03:04.792+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:03:04.792+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:03:04.805+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:03:04.805+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:03:04.823+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T16:03:34.967+0000] {processor.py:186} INFO - Started process (PID=2398) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:03:34.968+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:03:34.970+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:03:34.970+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:03:35.128+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:03:35.152+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:03:35.152+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:03:35.167+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:03:35.167+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:03:35.192+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.231 seconds
[2025-01-10T16:04:05.502+0000] {processor.py:186} INFO - Started process (PID=2405) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:04:05.502+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:04:05.505+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:04:05.504+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:04:05.649+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:04:05.671+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:04:05.671+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:04:05.684+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:04:05.684+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:04:05.707+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.212 seconds
[2025-01-10T16:04:35.890+0000] {processor.py:186} INFO - Started process (PID=2412) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:04:35.891+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:04:35.893+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:04:35.893+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:04:36.034+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:04:36.061+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:04:36.060+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:04:36.074+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:04:36.074+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:04:36.092+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T16:05:06.387+0000] {processor.py:186} INFO - Started process (PID=2419) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:05:06.388+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:05:06.390+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:05:06.390+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:05:06.548+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:05:06.572+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:05:06.572+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:05:06.586+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:05:06.585+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:05:06.603+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.221 seconds
[2025-01-10T16:05:36.738+0000] {processor.py:186} INFO - Started process (PID=2426) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:05:36.739+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:05:36.741+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:05:36.741+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:05:36.894+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:05:36.927+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:05:36.926+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:05:36.945+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:05:36.945+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:05:36.976+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.244 seconds
[2025-01-10T16:06:07.215+0000] {processor.py:186} INFO - Started process (PID=2433) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:06:07.216+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:06:07.219+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:06:07.218+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:06:07.356+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:06:07.377+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:06:07.376+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:06:07.389+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:06:07.389+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:06:07.404+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.194 seconds
[2025-01-10T16:06:37.533+0000] {processor.py:186} INFO - Started process (PID=2446) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:06:37.534+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:06:37.536+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:06:37.536+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:06:37.677+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:06:37.699+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:06:37.698+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:06:37.712+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:06:37.711+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:06:37.729+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T16:07:08.151+0000] {processor.py:186} INFO - Started process (PID=2453) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:07:08.152+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:07:08.155+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:07:08.155+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:07:08.305+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:07:08.328+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:07:08.327+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:07:08.342+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:07:08.342+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:07:08.383+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.238 seconds
[2025-01-10T16:07:38.529+0000] {processor.py:186} INFO - Started process (PID=2460) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:07:38.530+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:07:38.532+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:07:38.532+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:07:38.674+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:07:38.697+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:07:38.696+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:07:38.709+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:07:38.709+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:07:38.732+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T16:08:09.080+0000] {processor.py:186} INFO - Started process (PID=2467) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:08:09.081+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:08:09.083+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:08:09.083+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:08:09.232+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:08:09.254+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:08:09.254+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:08:09.268+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:08:09.268+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:08:09.285+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.210 seconds
[2025-01-10T16:08:39.511+0000] {processor.py:186} INFO - Started process (PID=2474) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:08:39.512+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:08:39.514+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:08:39.513+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:08:39.656+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:08:39.678+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:08:39.677+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:08:39.691+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:08:39.690+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:08:39.709+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T16:09:09.979+0000] {processor.py:186} INFO - Started process (PID=2481) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:09:09.980+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:09:09.982+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:09:09.982+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:09:10.124+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:09:10.146+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:09:10.146+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:09:10.159+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:09:10.159+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:09:10.176+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T16:09:40.309+0000] {processor.py:186} INFO - Started process (PID=2488) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:09:40.310+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:09:40.312+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:09:40.311+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:09:40.451+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:09:40.473+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:09:40.473+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:09:40.486+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:09:40.485+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:09:40.502+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.199 seconds
[2025-01-10T16:10:10.643+0000] {processor.py:186} INFO - Started process (PID=2495) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:10:10.644+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:10:10.646+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:10:10.646+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:10:10.788+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:10:10.812+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:10:10.812+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:10:10.825+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:10:10.825+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:10:10.841+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T16:10:41.193+0000] {processor.py:186} INFO - Started process (PID=2503) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:10:41.194+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:10:41.196+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:10:41.196+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:10:41.338+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:10:41.360+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:10:41.360+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:10:41.373+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:10:41.373+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:10:41.397+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.210 seconds
[2025-01-10T16:11:11.558+0000] {processor.py:186} INFO - Started process (PID=2509) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:11:11.559+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:11:11.561+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:11:11.561+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:11:11.702+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:11:11.724+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:11:11.724+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:11:11.737+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:11:11.737+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:11:11.760+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T16:11:42.094+0000] {processor.py:186} INFO - Started process (PID=2517) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:11:42.095+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:11:42.097+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:11:42.097+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:11:42.237+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:11:42.259+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:11:42.258+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:11:42.271+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:11:42.271+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:11:42.296+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T16:12:12.423+0000] {processor.py:186} INFO - Started process (PID=2524) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:12:12.424+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:12:12.426+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:12:12.426+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:12:12.571+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:12:12.594+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:12:12.593+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:12:12.606+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:12:12.606+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:12:12.630+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.212 seconds
[2025-01-10T16:12:43.030+0000] {processor.py:186} INFO - Started process (PID=2531) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:12:43.030+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:12:43.032+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:12:43.032+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:12:43.173+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:12:43.194+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:12:43.194+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:12:43.207+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:12:43.207+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:12:43.231+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T16:13:13.324+0000] {processor.py:186} INFO - Started process (PID=2538) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:13:13.324+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:13:13.326+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:13:13.326+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:13:13.467+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:13:13.490+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:13:13.489+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:13:13.504+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:13:13.504+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:13:13.522+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T16:13:43.653+0000] {processor.py:186} INFO - Started process (PID=2545) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:13:43.654+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:13:43.656+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:13:43.656+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:13:43.796+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:13:43.818+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:13:43.818+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:13:43.831+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:13:43.831+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:13:43.847+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T16:14:14.098+0000] {processor.py:186} INFO - Started process (PID=2552) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:14:14.099+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:14:14.101+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:14:14.101+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:14:14.282+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:14:14.307+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:14:14.306+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:14:14.321+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:14:14.321+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:14:14.339+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.247 seconds
[2025-01-10T16:14:44.516+0000] {processor.py:186} INFO - Started process (PID=2559) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:14:44.517+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:14:44.519+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:14:44.519+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:14:44.703+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:14:44.726+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:14:44.725+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:14:44.741+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:14:44.741+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:14:44.759+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.249 seconds
[2025-01-10T16:15:15.006+0000] {processor.py:186} INFO - Started process (PID=2566) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:15:15.007+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:15:15.009+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:15:15.009+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:15:15.172+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:15:15.195+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:15:15.195+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:15:15.209+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:15:15.209+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:15:15.226+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.226 seconds
[2025-01-10T16:15:45.580+0000] {processor.py:186} INFO - Started process (PID=2573) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:15:45.581+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:15:45.583+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:15:45.583+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:15:45.730+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:15:45.751+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:15:45.751+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:15:45.764+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:15:45.764+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:15:45.786+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.211 seconds
[2025-01-10T16:16:15.985+0000] {processor.py:186} INFO - Started process (PID=2581) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:16:15.986+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:16:15.988+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:16:15.988+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:16:16.127+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:16:16.147+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:16:16.147+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:16:16.160+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:16:16.160+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:16:16.176+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.196 seconds
[2025-01-10T16:16:46.454+0000] {processor.py:186} INFO - Started process (PID=2589) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:16:46.454+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:16:46.456+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:16:46.456+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:16:46.601+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:16:46.623+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:16:46.622+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:16:46.635+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:16:46.635+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:16:46.652+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T16:17:16.957+0000] {processor.py:186} INFO - Started process (PID=2596) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:17:16.958+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:17:16.960+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:17:16.959+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:17:17.100+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:17:17.128+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:17:17.127+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:17:17.140+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:17:17.140+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:17:17.164+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.213 seconds
[2025-01-10T16:17:47.309+0000] {processor.py:186} INFO - Started process (PID=2603) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:17:47.310+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:17:47.312+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:17:47.312+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:17:47.456+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:17:47.478+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:17:47.478+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:17:47.492+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:17:47.491+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:17:47.509+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T16:18:17.870+0000] {processor.py:186} INFO - Started process (PID=2610) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:18:17.871+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:18:17.873+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:18:17.873+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:18:18.018+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:18:18.040+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:18:18.040+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:18:18.053+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:18:18.053+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:18:18.075+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.210 seconds
[2025-01-10T16:18:48.252+0000] {processor.py:186} INFO - Started process (PID=2617) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:18:48.253+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:18:48.255+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:18:48.255+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:18:48.393+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:18:48.416+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:18:48.415+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:18:48.429+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:18:48.428+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:18:48.452+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T16:19:18.817+0000] {processor.py:186} INFO - Started process (PID=2625) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:19:18.818+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:19:18.820+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:19:18.820+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:19:18.958+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:19:18.980+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:19:18.980+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:19:18.993+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:19:18.993+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:19:19.010+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.198 seconds
[2025-01-10T16:19:49.159+0000] {processor.py:186} INFO - Started process (PID=2632) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:19:49.160+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:19:49.163+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:19:49.162+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:19:49.304+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:19:49.326+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:19:49.326+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:19:49.340+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:19:49.340+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:19:49.362+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.210 seconds
[2025-01-10T16:20:19.755+0000] {processor.py:186} INFO - Started process (PID=2639) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:20:19.756+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:20:19.758+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:20:19.758+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:20:19.896+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:20:19.917+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:20:19.917+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:20:19.930+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:20:19.930+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:20:19.953+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T16:20:50.051+0000] {processor.py:186} INFO - Started process (PID=2647) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:20:50.051+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:20:50.054+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:20:50.053+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:20:50.193+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:20:50.216+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:20:50.215+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:20:50.228+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:20:50.228+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:20:50.253+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T16:21:20.596+0000] {processor.py:186} INFO - Started process (PID=2654) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:21:20.597+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:21:20.600+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:21:20.599+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:21:20.745+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:21:20.767+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:21:20.767+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:21:20.780+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:21:20.780+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:21:20.802+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.211 seconds
[2025-01-10T16:21:51.023+0000] {processor.py:186} INFO - Started process (PID=2661) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:21:51.024+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:21:51.026+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:21:51.026+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:21:51.166+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:21:51.187+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:21:51.186+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:21:51.199+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:21:51.199+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:21:51.231+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.214 seconds
[2025-01-10T16:22:21.513+0000] {processor.py:186} INFO - Started process (PID=2668) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:22:21.516+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:22:21.523+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:22:21.522+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:22:21.810+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:22:21.854+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:22:21.854+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:22:21.886+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:22:21.886+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:22:21.915+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.411 seconds
[2025-01-10T16:22:51.984+0000] {processor.py:186} INFO - Started process (PID=2675) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:22:51.985+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:22:51.988+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:22:51.987+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:22:52.130+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:22:52.155+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:22:52.154+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:22:52.169+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:22:52.169+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:22:52.186+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T16:23:22.349+0000] {processor.py:186} INFO - Started process (PID=2682) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:23:22.350+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:23:22.353+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:23:22.352+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:23:22.493+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:23:22.517+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:23:22.516+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:23:22.530+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:23:22.529+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:23:22.552+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T16:23:52.934+0000] {processor.py:186} INFO - Started process (PID=2690) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:23:52.935+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:23:52.937+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:23:52.937+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:23:53.074+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:23:53.095+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:23:53.094+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:23:53.107+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:23:53.107+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:23:53.123+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.194 seconds
[2025-01-10T16:24:23.326+0000] {processor.py:186} INFO - Started process (PID=2698) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:24:23.327+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:24:23.329+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:24:23.329+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:24:23.472+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:24:23.494+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:24:23.493+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:24:23.507+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:24:23.507+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:24:23.524+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T16:24:53.786+0000] {processor.py:186} INFO - Started process (PID=2706) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:24:53.787+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:24:53.789+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:24:53.789+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:24:53.932+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:24:53.953+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:24:53.953+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:24:53.966+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:24:53.965+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:24:53.983+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T16:25:24.223+0000] {processor.py:186} INFO - Started process (PID=2712) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:25:24.224+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:25:24.226+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:25:24.226+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:25:24.364+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:25:24.386+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:25:24.386+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:25:24.399+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:25:24.399+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:25:24.422+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T16:25:54.684+0000] {processor.py:186} INFO - Started process (PID=2719) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:25:54.685+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:25:54.687+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:25:54.687+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:25:54.828+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:25:54.849+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:25:54.849+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:25:54.862+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:25:54.862+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:25:54.886+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T16:26:25.127+0000] {processor.py:186} INFO - Started process (PID=2726) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:26:25.128+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:26:25.131+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:26:25.130+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:26:25.276+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:26:25.297+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:26:25.296+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:26:25.310+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:26:25.310+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:26:25.330+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T16:26:55.453+0000] {processor.py:186} INFO - Started process (PID=2733) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:26:55.454+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:26:55.456+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:26:55.455+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:26:55.595+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:26:55.618+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:26:55.617+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:26:55.630+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:26:55.630+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:26:55.652+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T16:27:26.034+0000] {processor.py:186} INFO - Started process (PID=2740) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:27:26.035+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:27:26.037+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:27:26.037+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:27:26.182+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:27:26.204+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:27:26.204+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:27:26.217+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:27:26.217+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:27:26.234+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T16:27:56.347+0000] {processor.py:186} INFO - Started process (PID=2746) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:27:56.348+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:27:56.350+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:27:56.350+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:27:56.488+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:27:56.512+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:27:56.512+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:27:56.526+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:27:56.525+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:27:56.548+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T16:28:26.914+0000] {processor.py:186} INFO - Started process (PID=2753) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:28:26.914+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:28:26.916+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:28:26.916+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:28:27.057+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:28:27.078+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:28:27.078+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:28:27.091+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:28:27.091+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:28:27.110+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T16:28:57.238+0000] {processor.py:186} INFO - Started process (PID=2760) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:28:57.239+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:28:57.241+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:28:57.240+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:28:57.383+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:28:57.406+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:28:57.406+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:28:57.421+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:28:57.420+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:28:57.439+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T16:29:27.889+0000] {processor.py:186} INFO - Started process (PID=2768) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:29:27.890+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:29:27.892+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:29:27.892+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:29:28.041+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:29:28.064+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:29:28.063+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:29:28.077+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:29:28.077+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:29:28.104+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.220 seconds
[2025-01-10T16:29:58.242+0000] {processor.py:186} INFO - Started process (PID=2775) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:29:58.243+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:29:58.246+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:29:58.245+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:29:58.411+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:29:58.433+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:29:58.433+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:29:58.451+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:29:58.451+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:29:58.472+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.236 seconds
[2025-01-10T16:30:28.852+0000] {processor.py:186} INFO - Started process (PID=2783) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:30:28.853+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:30:28.855+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:30:28.854+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:30:28.993+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:30:29.018+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:30:29.017+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:30:29.030+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:30:29.030+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:30:29.047+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T16:30:59.215+0000] {processor.py:186} INFO - Started process (PID=2790) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:30:59.215+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:30:59.217+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:30:59.217+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:30:59.363+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:30:59.386+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:30:59.385+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:30:59.399+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:30:59.398+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:30:59.415+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T16:31:29.808+0000] {processor.py:186} INFO - Started process (PID=2797) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:31:29.809+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:31:29.811+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:31:29.811+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:31:29.950+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:31:29.971+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:31:29.971+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:31:29.983+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:31:29.983+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:31:30.000+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.196 seconds
[2025-01-10T16:32:00.208+0000] {processor.py:186} INFO - Started process (PID=2804) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:32:00.209+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:32:00.211+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:32:00.211+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:32:00.354+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:32:00.376+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:32:00.376+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:32:00.389+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:32:00.389+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:32:00.407+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T16:32:30.649+0000] {processor.py:186} INFO - Started process (PID=2812) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:32:30.650+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:32:30.652+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:32:30.652+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:32:30.793+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:32:30.815+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:32:30.814+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:32:30.828+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:32:30.827+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:32:30.851+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T16:33:01.127+0000] {processor.py:186} INFO - Started process (PID=2819) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:33:01.128+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:33:01.130+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:33:01.130+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:33:01.273+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:33:01.294+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:33:01.293+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:33:01.307+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:33:01.306+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:33:01.331+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.210 seconds
[2025-01-10T16:33:31.528+0000] {processor.py:186} INFO - Started process (PID=2826) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:33:31.529+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:33:31.531+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:33:31.530+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:33:31.672+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:33:31.698+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:33:31.697+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:33:31.711+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:33:31.711+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:33:31.732+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.210 seconds
[2025-01-10T16:34:02.077+0000] {processor.py:186} INFO - Started process (PID=2834) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:34:02.078+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:34:02.082+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:34:02.082+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:34:02.242+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:34:02.269+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:34:02.269+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:34:02.282+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:34:02.282+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:34:02.305+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.240 seconds
[2025-01-10T16:34:32.388+0000] {processor.py:186} INFO - Started process (PID=2841) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:34:32.389+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:34:32.391+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:34:32.391+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:34:32.530+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:34:32.559+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:34:32.558+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:34:32.572+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:34:32.572+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:34:32.590+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T16:35:02.748+0000] {processor.py:186} INFO - Started process (PID=2848) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:35:02.749+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:35:02.752+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:35:02.751+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:35:02.901+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:35:02.922+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:35:02.922+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:35:02.935+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:35:02.935+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:35:02.953+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.213 seconds
[2025-01-10T16:35:33.231+0000] {processor.py:186} INFO - Started process (PID=2855) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:35:33.232+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:35:33.234+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:35:33.234+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:35:33.376+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:35:33.398+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:35:33.398+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:35:33.411+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:35:33.411+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:35:33.428+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T16:36:03.559+0000] {processor.py:186} INFO - Started process (PID=2862) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:36:03.560+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:36:03.563+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:36:03.562+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:36:03.703+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:36:03.725+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:36:03.725+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:36:03.738+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:36:03.738+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:36:03.755+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T16:36:33.951+0000] {processor.py:186} INFO - Started process (PID=2869) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:36:33.952+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:36:33.954+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:36:33.954+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:36:34.092+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:36:34.114+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:36:34.114+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:36:34.126+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:36:34.126+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:36:34.147+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T16:37:04.449+0000] {processor.py:186} INFO - Started process (PID=2877) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:37:04.449+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:37:04.452+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:37:04.452+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:37:04.621+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:37:04.644+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:37:04.644+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:37:04.663+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:37:04.663+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:37:04.682+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.239 seconds
[2025-01-10T16:37:34.900+0000] {processor.py:186} INFO - Started process (PID=2884) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:37:34.901+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:37:34.903+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:37:34.903+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:37:35.043+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:37:35.064+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:37:35.064+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:37:35.077+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:37:35.077+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:37:35.112+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.217 seconds
[2025-01-10T16:38:05.370+0000] {processor.py:186} INFO - Started process (PID=2892) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:38:05.371+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:38:05.373+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:38:05.373+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:38:05.519+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:38:05.541+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:38:05.541+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:38:05.554+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:38:05.554+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:38:05.579+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.214 seconds
[2025-01-10T16:38:35.895+0000] {processor.py:186} INFO - Started process (PID=2899) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:38:35.896+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:38:35.899+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:38:35.898+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:38:36.060+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:38:36.082+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:38:36.082+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:38:36.106+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:38:36.106+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:38:36.138+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.248 seconds
[2025-01-10T16:39:06.273+0000] {processor.py:186} INFO - Started process (PID=2906) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:39:06.274+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:39:06.276+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:39:06.276+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:39:06.417+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:39:06.439+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:39:06.439+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:39:06.459+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:39:06.459+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:39:06.476+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T16:39:36.602+0000] {processor.py:186} INFO - Started process (PID=2913) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:39:36.603+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:39:36.605+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:39:36.605+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:39:36.749+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:39:36.771+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:39:36.771+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:39:36.784+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:39:36.784+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:39:36.802+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T16:40:06.927+0000] {processor.py:186} INFO - Started process (PID=2921) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:40:06.928+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:40:06.930+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:40:06.930+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:40:07.071+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:40:07.093+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:40:07.093+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:40:07.106+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:40:07.106+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:40:07.124+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T16:40:37.231+0000] {processor.py:186} INFO - Started process (PID=2928) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:40:37.231+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:40:37.234+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:40:37.233+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:40:37.377+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:40:37.398+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:40:37.398+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:40:37.411+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:40:37.411+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:40:37.428+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T16:41:07.806+0000] {processor.py:186} INFO - Started process (PID=2935) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:41:07.806+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:41:07.808+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:41:07.808+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:41:07.948+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:41:07.970+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:41:07.969+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:41:07.982+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:41:07.982+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:41:07.999+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.199 seconds
[2025-01-10T16:41:38.178+0000] {processor.py:186} INFO - Started process (PID=2943) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:41:38.179+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:41:38.182+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:41:38.181+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:41:38.319+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:41:38.343+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:41:38.342+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:41:38.355+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:41:38.355+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:41:38.377+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T16:42:08.697+0000] {processor.py:186} INFO - Started process (PID=2950) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:42:08.698+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:42:08.700+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:42:08.700+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:42:08.845+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:42:08.866+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:42:08.866+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:42:08.879+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:42:08.879+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:42:08.901+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T16:42:39.032+0000] {processor.py:186} INFO - Started process (PID=2957) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:42:39.033+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:42:39.036+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:42:39.035+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:42:39.178+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:42:39.200+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:42:39.200+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:42:39.213+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:42:39.213+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:42:39.230+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T16:43:09.597+0000] {processor.py:186} INFO - Started process (PID=2965) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:43:09.598+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:43:09.600+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:43:09.600+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:43:09.742+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:43:09.763+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:43:09.762+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:43:09.776+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:43:09.776+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:43:09.793+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T16:43:39.949+0000] {processor.py:186} INFO - Started process (PID=2973) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:43:39.950+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:43:39.953+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:43:39.952+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:43:40.108+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:43:40.131+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:43:40.130+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:43:40.144+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:43:40.143+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:43:40.161+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.219 seconds
[2025-01-10T16:44:10.356+0000] {processor.py:186} INFO - Started process (PID=2988) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:44:10.357+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:44:10.360+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:44:10.359+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:44:10.498+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:44:10.521+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:44:10.520+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:44:10.534+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:44:10.534+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:44:10.551+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T16:44:40.895+0000] {processor.py:186} INFO - Started process (PID=2995) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:44:40.896+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:44:40.899+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:44:40.898+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:44:41.081+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:44:41.103+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:44:41.102+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:44:41.115+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:44:41.115+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:44:41.138+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.249 seconds
[2025-01-10T16:45:11.195+0000] {processor.py:186} INFO - Started process (PID=3002) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:45:11.196+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:45:11.198+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:45:11.198+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:45:11.346+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:45:11.368+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:45:11.367+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:45:11.381+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:45:11.381+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:45:11.405+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.216 seconds
[2025-01-10T16:45:41.526+0000] {processor.py:186} INFO - Started process (PID=3009) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:45:41.527+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:45:41.529+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:45:41.529+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:45:41.671+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:45:41.693+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:45:41.693+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:45:41.706+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:45:41.706+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:45:41.727+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T16:46:11.937+0000] {processor.py:186} INFO - Started process (PID=3017) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:46:11.938+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:46:11.940+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:46:11.940+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:46:12.082+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:46:12.103+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:46:12.102+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:46:12.115+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:46:12.115+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:46:12.138+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T16:46:42.312+0000] {processor.py:186} INFO - Started process (PID=3024) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:46:42.312+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:46:42.315+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:46:42.314+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:46:42.461+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:46:42.486+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:46:42.485+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:46:42.500+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:46:42.500+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:46:42.516+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.210 seconds
[2025-01-10T16:47:12.852+0000] {processor.py:186} INFO - Started process (PID=3031) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:47:12.853+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:47:12.855+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:47:12.855+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:47:13.033+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:47:13.055+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:47:13.055+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:47:13.069+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:47:13.069+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:47:13.087+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.242 seconds
[2025-01-10T16:47:43.197+0000] {processor.py:186} INFO - Started process (PID=3039) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:47:43.199+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:47:43.203+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:47:43.203+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:47:43.348+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:47:43.375+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:47:43.374+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:47:43.390+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:47:43.390+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:47:43.412+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.227 seconds
[2025-01-10T16:48:13.529+0000] {processor.py:186} INFO - Started process (PID=3046) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:48:13.530+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:48:13.533+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:48:13.532+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:48:13.674+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:48:13.696+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:48:13.696+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:48:13.709+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:48:13.709+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:48:13.732+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T16:48:44.029+0000] {processor.py:186} INFO - Started process (PID=3052) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:48:44.030+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:48:44.032+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:48:44.032+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:48:44.178+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:48:44.201+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:48:44.200+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:48:44.213+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:48:44.213+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:48:44.229+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T16:49:14.341+0000] {processor.py:186} INFO - Started process (PID=3058) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:49:14.341+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:49:14.344+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:49:14.343+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:49:14.483+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:49:14.506+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:49:14.505+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:49:14.519+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:49:14.519+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:49:14.537+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T16:49:44.711+0000] {processor.py:186} INFO - Started process (PID=3065) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:49:44.712+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:49:44.714+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:49:44.714+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:49:44.855+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:49:44.878+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:49:44.878+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:49:44.891+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:49:44.891+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:49:44.907+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T16:50:15.233+0000] {processor.py:186} INFO - Started process (PID=3073) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:50:15.234+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:50:15.236+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:50:15.236+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:50:15.375+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:50:15.396+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:50:15.396+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:50:15.409+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:50:15.409+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:50:15.433+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T16:50:45.593+0000] {processor.py:186} INFO - Started process (PID=3081) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:50:45.594+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:50:45.596+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:50:45.596+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:50:45.736+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:50:45.758+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:50:45.758+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:50:45.771+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:50:45.771+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:50:45.787+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T16:51:16.081+0000] {processor.py:186} INFO - Started process (PID=3088) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:51:16.082+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:51:16.085+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:51:16.085+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:51:16.230+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:51:16.252+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:51:16.252+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:51:16.265+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:51:16.265+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:51:16.284+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T16:51:46.466+0000] {processor.py:186} INFO - Started process (PID=3096) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:51:46.467+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:51:46.470+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:51:46.469+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:51:46.615+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:51:46.638+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:51:46.637+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:51:46.652+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:51:46.652+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:51:46.683+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.223 seconds
[2025-01-10T16:52:17.002+0000] {processor.py:186} INFO - Started process (PID=3103) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:52:17.003+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:52:17.005+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:52:17.005+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:52:17.147+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:52:17.167+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:52:17.167+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:52:17.180+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:52:17.180+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:52:17.196+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T16:52:47.364+0000] {processor.py:186} INFO - Started process (PID=3110) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:52:47.365+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:52:47.368+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:52:47.367+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:52:47.547+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:52:47.573+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:52:47.573+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:52:47.586+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:52:47.586+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:52:47.603+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.245 seconds
[2025-01-10T16:53:17.877+0000] {processor.py:186} INFO - Started process (PID=3117) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:53:17.878+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:53:17.880+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:53:17.879+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:53:18.020+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:53:18.041+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:53:18.041+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:53:18.054+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:53:18.053+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:53:18.072+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T16:53:48.366+0000] {processor.py:186} INFO - Started process (PID=3124) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:53:48.367+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:53:48.369+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:53:48.368+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:53:48.509+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:53:48.531+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:53:48.531+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:53:48.544+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:53:48.544+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:53:48.567+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T16:54:18.755+0000] {processor.py:186} INFO - Started process (PID=3131) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:54:18.756+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:54:18.758+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:54:18.758+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:54:18.899+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:54:18.920+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:54:18.920+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:54:18.933+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:54:18.933+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:54:18.956+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T16:54:49.100+0000] {processor.py:186} INFO - Started process (PID=3139) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:54:49.101+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:54:49.103+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:54:49.102+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:54:49.243+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:54:49.266+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:54:49.265+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:54:49.279+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:54:49.279+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:54:49.302+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T16:55:19.639+0000] {processor.py:186} INFO - Started process (PID=3146) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:55:19.640+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:55:19.642+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:55:19.642+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:55:19.799+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:55:19.821+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:55:19.821+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:55:19.836+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:55:19.835+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:55:19.862+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.229 seconds
[2025-01-10T16:55:50.042+0000] {processor.py:186} INFO - Started process (PID=3153) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:55:50.043+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:55:50.045+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:55:50.044+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:55:50.198+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:55:50.224+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:55:50.223+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:55:50.238+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:55:50.237+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:55:50.260+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.224 seconds
[2025-01-10T16:56:20.541+0000] {processor.py:186} INFO - Started process (PID=3160) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:56:20.542+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:56:20.544+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:56:20.544+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:56:20.692+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:56:20.714+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:56:20.714+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:56:20.727+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:56:20.727+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:56:20.743+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T16:56:50.956+0000] {processor.py:186} INFO - Started process (PID=3167) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:56:50.957+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:56:50.959+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:56:50.958+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:56:51.105+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:56:51.127+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:56:51.126+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:56:51.140+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:56:51.140+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:56:51.156+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T16:57:21.446+0000] {processor.py:186} INFO - Started process (PID=3174) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:57:21.447+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:57:21.449+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:57:21.448+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:57:21.590+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:57:21.619+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:57:21.618+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:57:21.631+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:57:21.631+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:57:21.653+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.213 seconds
[2025-01-10T16:57:51.928+0000] {processor.py:186} INFO - Started process (PID=3181) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:57:51.929+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:57:51.931+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:57:51.931+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:57:52.081+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:57:52.103+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:57:52.103+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:57:52.116+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:57:52.116+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:57:52.139+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.217 seconds
[2025-01-10T16:58:22.354+0000] {processor.py:186} INFO - Started process (PID=3188) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:58:22.355+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:58:22.357+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:58:22.357+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:58:22.505+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:58:22.527+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:58:22.527+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:58:22.540+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:58:22.540+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:58:22.557+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T16:58:52.789+0000] {processor.py:186} INFO - Started process (PID=3195) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:58:52.790+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:58:52.792+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:58:52.792+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:58:52.937+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:58:52.959+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:58:52.958+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:58:52.972+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:58:52.972+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:58:52.989+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T16:59:23.169+0000] {processor.py:186} INFO - Started process (PID=3202) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:59:23.170+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:59:23.172+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:59:23.172+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:59:23.312+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:59:23.333+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:59:23.333+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:59:23.346+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:59:23.346+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:59:23.364+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T16:59:53.538+0000] {processor.py:186} INFO - Started process (PID=3209) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T16:59:53.539+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T16:59:53.542+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:59:53.541+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:59:53.681+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T16:59:53.704+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:59:53.704+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T16:59:53.717+0000] {logging_mixin.py:190} INFO - [2025-01-10T16:59:53.717+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T16:59:53.735+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T17:00:23.826+0000] {processor.py:186} INFO - Started process (PID=3216) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:00:23.827+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:00:23.831+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:00:23.831+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:00:23.985+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:00:24.007+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:00:24.006+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:00:24.021+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:00:24.020+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:00:24.040+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.219 seconds
[2025-01-10T17:00:54.145+0000] {processor.py:186} INFO - Started process (PID=3223) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:00:54.146+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:00:54.148+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:00:54.148+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:00:54.292+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:00:54.314+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:00:54.314+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:00:54.327+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:00:54.327+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:00:54.345+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T17:01:24.456+0000] {processor.py:186} INFO - Started process (PID=3231) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:01:24.457+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:01:24.459+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:01:24.459+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:01:24.601+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:01:24.623+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:01:24.623+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:01:24.636+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:01:24.636+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:01:24.676+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.225 seconds
[2025-01-10T17:01:54.824+0000] {processor.py:186} INFO - Started process (PID=3239) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:01:54.825+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:01:54.827+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:01:54.827+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:01:54.969+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:01:54.991+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:01:54.990+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:01:55.004+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:01:55.004+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:01:55.021+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T17:02:25.124+0000] {processor.py:186} INFO - Started process (PID=3246) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:02:25.125+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:02:25.127+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:02:25.127+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:02:25.272+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:02:25.293+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:02:25.293+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:02:25.306+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:02:25.306+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:02:25.323+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T17:02:55.726+0000] {processor.py:186} INFO - Started process (PID=3253) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:02:55.727+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:02:55.729+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:02:55.729+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:02:55.870+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:02:55.892+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:02:55.891+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:02:55.905+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:02:55.904+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:02:55.928+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T17:03:26.058+0000] {processor.py:186} INFO - Started process (PID=3260) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:03:26.059+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:03:26.062+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:03:26.061+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:03:26.201+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:03:26.223+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:03:26.222+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:03:26.235+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:03:26.235+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:03:26.259+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T17:03:56.519+0000] {processor.py:186} INFO - Started process (PID=3268) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:03:56.520+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:03:56.522+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:03:56.522+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:03:56.689+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:03:56.714+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:03:56.714+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:03:56.727+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:03:56.727+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:03:56.750+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.237 seconds
[2025-01-10T17:04:26.899+0000] {processor.py:186} INFO - Started process (PID=3275) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:04:26.900+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:04:26.903+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:04:26.902+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:04:27.041+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:04:27.063+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:04:27.063+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:04:27.076+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:04:27.076+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:04:27.099+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T17:04:57.466+0000] {processor.py:186} INFO - Started process (PID=3281) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:04:57.467+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:04:57.469+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:04:57.469+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:04:57.613+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:04:57.634+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:04:57.634+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:04:57.648+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:04:57.648+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:04:57.665+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T17:05:27.836+0000] {processor.py:186} INFO - Started process (PID=3288) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:05:27.837+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:05:27.839+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:05:27.839+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:05:27.978+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:05:27.998+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:05:27.998+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:05:28.011+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:05:28.011+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:05:28.027+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.197 seconds
[2025-01-10T17:05:58.254+0000] {processor.py:186} INFO - Started process (PID=3295) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:05:58.255+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:05:58.257+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:05:58.257+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:05:58.395+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:05:58.417+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:05:58.417+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:05:58.430+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:05:58.429+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:05:58.453+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T17:06:28.624+0000] {processor.py:186} INFO - Started process (PID=3302) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:06:28.624+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:06:28.627+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:06:28.627+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:06:28.764+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:06:28.786+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:06:28.786+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:06:28.799+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:06:28.799+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:06:28.823+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T17:06:59.182+0000] {processor.py:186} INFO - Started process (PID=3309) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:06:59.182+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:06:59.185+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:06:59.184+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:06:59.326+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:06:59.347+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:06:59.347+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:06:59.360+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:06:59.360+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:06:59.382+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T17:07:29.562+0000] {processor.py:186} INFO - Started process (PID=3315) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:07:29.563+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:07:29.566+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:07:29.565+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:07:29.706+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:07:29.728+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:07:29.728+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:07:29.741+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:07:29.741+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:07:29.765+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T17:08:00.071+0000] {processor.py:186} INFO - Started process (PID=3322) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:08:00.076+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:08:00.079+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:08:00.078+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:08:00.244+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:08:00.267+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:08:00.266+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:08:00.280+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:08:00.280+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:08:00.303+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.245 seconds
[2025-01-10T17:08:30.428+0000] {processor.py:186} INFO - Started process (PID=3329) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:08:30.429+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:08:30.431+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:08:30.430+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:08:30.571+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:08:30.594+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:08:30.593+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:08:30.607+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:08:30.606+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:08:30.624+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T17:09:00.765+0000] {processor.py:186} INFO - Started process (PID=3337) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:09:00.766+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:09:00.768+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:09:00.767+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:09:00.907+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:09:00.928+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:09:00.928+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:09:00.941+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:09:00.941+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:09:00.957+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.199 seconds
[2025-01-10T17:09:31.022+0000] {processor.py:186} INFO - Started process (PID=3344) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:09:31.023+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:09:31.025+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:09:31.025+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:09:31.167+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:09:31.190+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:09:31.189+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:09:31.203+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:09:31.202+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:09:31.220+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T17:10:01.331+0000] {processor.py:186} INFO - Started process (PID=3351) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:10:01.332+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:10:01.334+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:10:01.334+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:10:01.474+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:10:01.496+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:10:01.496+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:10:01.509+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:10:01.509+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:10:01.527+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T17:10:31.623+0000] {processor.py:186} INFO - Started process (PID=3358) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:10:31.624+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:10:31.626+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:10:31.626+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:10:31.765+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:10:31.787+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:10:31.787+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:10:31.800+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:10:31.800+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:10:31.817+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T17:11:01.994+0000] {processor.py:186} INFO - Started process (PID=3365) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:11:01.995+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:11:01.998+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:11:01.997+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:11:02.143+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:11:02.165+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:11:02.164+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:11:02.177+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:11:02.177+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:11:02.201+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.214 seconds
[2025-01-10T17:11:32.470+0000] {processor.py:186} INFO - Started process (PID=3372) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:11:32.471+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:11:32.473+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:11:32.473+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:11:32.618+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:11:32.640+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:11:32.639+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:11:32.653+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:11:32.653+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:11:32.671+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T17:12:02.885+0000] {processor.py:186} INFO - Started process (PID=3379) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:12:02.886+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:12:02.888+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:12:02.888+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:12:03.047+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:12:03.068+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:12:03.067+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:12:03.082+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:12:03.081+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:12:03.103+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.224 seconds
[2025-01-10T17:12:33.407+0000] {processor.py:186} INFO - Started process (PID=3386) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:12:33.408+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:12:33.410+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:12:33.410+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:12:33.581+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:12:33.603+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:12:33.603+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:12:33.618+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:12:33.618+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:12:33.641+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.241 seconds
[2025-01-10T17:13:03.844+0000] {processor.py:186} INFO - Started process (PID=3393) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:13:03.847+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:13:03.852+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:13:03.852+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:13:04.009+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:13:04.030+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:13:04.030+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:13:04.043+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:13:04.043+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:13:04.059+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.223 seconds
[2025-01-10T17:13:34.212+0000] {processor.py:186} INFO - Started process (PID=3400) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:13:34.213+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:13:34.216+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:13:34.215+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:13:34.360+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:13:34.399+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:13:34.398+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:13:34.412+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:13:34.411+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:13:34.429+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.223 seconds
[2025-01-10T17:14:04.681+0000] {processor.py:186} INFO - Started process (PID=3408) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:14:04.682+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:14:04.685+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:14:04.684+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:14:04.854+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:14:04.880+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:14:04.880+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:14:04.896+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:14:04.895+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:14:04.915+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.240 seconds
[2025-01-10T17:14:35.029+0000] {processor.py:186} INFO - Started process (PID=3416) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:14:35.031+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:14:35.033+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:14:35.033+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:14:35.174+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:14:35.195+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:14:35.195+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:14:35.208+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:14:35.208+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:14:35.224+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T17:15:05.529+0000] {processor.py:186} INFO - Started process (PID=3423) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:15:05.530+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:15:05.532+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:15:05.532+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:15:05.675+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:15:05.696+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:15:05.695+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:15:05.708+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:15:05.708+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:15:05.725+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T17:15:35.837+0000] {processor.py:186} INFO - Started process (PID=3430) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:15:35.838+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:15:35.840+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:15:35.840+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:15:35.991+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:15:36.014+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:15:36.013+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:15:36.029+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:15:36.028+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:15:36.053+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.221 seconds
[2025-01-10T17:16:06.191+0000] {processor.py:186} INFO - Started process (PID=3438) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:16:06.192+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:16:06.194+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:16:06.194+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:16:06.336+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:16:06.359+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:16:06.359+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:16:06.373+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:16:06.372+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:16:06.391+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T17:16:36.824+0000] {processor.py:186} INFO - Started process (PID=3445) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:16:36.825+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:16:36.827+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:16:36.826+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:16:36.968+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:16:36.989+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:16:36.989+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:16:37.002+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:16:37.002+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:16:37.027+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T17:17:07.124+0000] {processor.py:186} INFO - Started process (PID=3453) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:17:07.125+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:17:07.127+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:17:07.127+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:17:07.281+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:17:07.305+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:17:07.304+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:17:07.318+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:17:07.318+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:17:07.335+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.216 seconds
[2025-01-10T17:17:37.507+0000] {processor.py:186} INFO - Started process (PID=3460) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:17:37.508+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:17:37.510+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:17:37.510+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:17:37.652+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:17:37.673+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:17:37.672+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:17:37.685+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:17:37.685+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:17:37.702+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T17:18:08.051+0000] {processor.py:186} INFO - Started process (PID=3467) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:18:08.052+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:18:08.054+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:18:08.054+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:18:08.196+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:18:08.218+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:18:08.218+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:18:08.231+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:18:08.231+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:18:08.248+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T17:18:38.391+0000] {processor.py:186} INFO - Started process (PID=3474) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:18:38.392+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:18:38.394+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:18:38.394+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:18:38.532+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:18:38.555+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:18:38.555+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:18:38.569+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:18:38.569+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:18:38.586+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T17:19:08.935+0000] {processor.py:186} INFO - Started process (PID=3481) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:19:08.936+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:19:08.938+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:19:08.938+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:19:09.079+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:19:09.101+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:19:09.100+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:19:09.113+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:19:09.113+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:19:09.145+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.215 seconds
[2025-01-10T17:19:39.289+0000] {processor.py:186} INFO - Started process (PID=3488) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:19:39.290+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:19:39.293+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:19:39.292+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:19:39.435+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:19:39.457+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:19:39.456+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:19:39.469+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:19:39.469+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:19:39.488+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T17:20:09.817+0000] {processor.py:186} INFO - Started process (PID=3496) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:20:09.818+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:20:09.820+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:20:09.820+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:20:09.967+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:20:09.990+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:20:09.989+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:20:10.003+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:20:10.003+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:20:10.020+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T17:20:40.176+0000] {processor.py:186} INFO - Started process (PID=3504) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:20:40.177+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:20:40.179+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:20:40.179+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:20:40.354+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:20:40.376+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:20:40.375+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:20:40.390+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:20:40.390+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:20:40.413+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.244 seconds
[2025-01-10T17:21:10.692+0000] {processor.py:186} INFO - Started process (PID=3510) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:21:10.693+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:21:10.695+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:21:10.695+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:21:10.837+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:21:10.858+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:21:10.858+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:21:10.871+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:21:10.871+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:21:10.887+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T17:21:41.065+0000] {processor.py:186} INFO - Started process (PID=3517) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:21:41.065+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:21:41.068+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:21:41.067+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:21:41.213+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:21:41.235+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:21:41.234+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:21:41.248+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:21:41.248+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:21:41.266+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T17:22:11.550+0000] {processor.py:186} INFO - Started process (PID=3524) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:22:11.551+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:22:11.553+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:22:11.553+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:22:11.696+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:22:11.717+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:22:11.717+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:22:11.730+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:22:11.730+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:22:11.746+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T17:22:41.928+0000] {processor.py:186} INFO - Started process (PID=3531) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:22:41.929+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:22:41.931+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:22:41.931+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:22:42.074+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:22:42.096+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:22:42.095+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:22:42.108+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:22:42.108+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:22:42.129+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T17:23:12.441+0000] {processor.py:186} INFO - Started process (PID=3538) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:23:12.442+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:23:12.446+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:23:12.445+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:23:12.603+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:23:12.629+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:23:12.629+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:23:12.660+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:23:12.660+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:23:12.709+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.273 seconds
[2025-01-10T17:23:42.815+0000] {processor.py:186} INFO - Started process (PID=3551) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:23:42.816+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:23:42.818+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:23:42.818+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:23:43.019+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:23:43.043+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:23:43.043+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:23:43.064+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:23:43.063+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:23:43.083+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.273 seconds
[2025-01-10T17:24:13.450+0000] {processor.py:186} INFO - Started process (PID=3558) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:24:13.451+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:24:13.453+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:24:13.453+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:24:13.611+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:24:13.638+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:24:13.638+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:24:13.651+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:24:13.651+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:24:13.668+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.223 seconds
[2025-01-10T17:24:43.807+0000] {processor.py:186} INFO - Started process (PID=3566) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:24:43.808+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:24:43.813+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:24:43.812+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:24:43.956+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:24:43.978+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:24:43.977+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:24:43.990+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:24:43.990+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:24:44.013+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.212 seconds
[2025-01-10T17:25:14.349+0000] {processor.py:186} INFO - Started process (PID=3573) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:25:14.350+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:25:14.352+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:25:14.351+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:25:14.492+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:25:14.513+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:25:14.513+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:25:14.526+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:25:14.526+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:25:14.549+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T17:25:44.672+0000] {processor.py:186} INFO - Started process (PID=3580) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:25:44.673+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:25:44.675+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:25:44.675+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:25:44.815+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:25:44.844+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:25:44.843+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:25:44.856+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:25:44.856+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:25:44.875+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.219 seconds
[2025-01-10T17:26:15.225+0000] {processor.py:186} INFO - Started process (PID=3587) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:26:15.225+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:26:15.228+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:26:15.227+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:26:15.370+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:26:15.391+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:26:15.391+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:26:15.404+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:26:15.404+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:26:15.427+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T17:26:45.572+0000] {processor.py:186} INFO - Started process (PID=3594) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:26:45.572+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:26:45.575+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:26:45.574+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:26:45.713+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:26:45.736+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:26:45.735+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:26:45.749+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:26:45.748+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:26:45.771+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T17:27:15.959+0000] {processor.py:186} INFO - Started process (PID=3601) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:27:15.960+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:27:15.962+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:27:15.962+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:27:16.102+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:27:16.124+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:27:16.123+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:27:16.137+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:27:16.136+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:27:16.159+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T17:27:46.526+0000] {processor.py:186} INFO - Started process (PID=3608) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:27:46.527+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:27:46.529+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:27:46.529+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:27:46.672+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:27:46.696+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:27:46.695+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:27:46.709+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:27:46.709+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:27:46.726+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T17:28:16.852+0000] {processor.py:186} INFO - Started process (PID=3615) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:28:16.853+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:28:16.855+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:28:16.855+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:28:16.995+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:28:17.019+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:28:17.018+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:28:17.033+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:28:17.033+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:28:17.049+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T17:28:47.328+0000] {processor.py:186} INFO - Started process (PID=3622) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:28:47.329+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:28:47.332+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:28:47.331+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:28:47.488+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:28:47.512+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:28:47.511+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:28:47.525+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:28:47.525+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:28:47.547+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.225 seconds
[2025-01-10T17:29:17.633+0000] {processor.py:186} INFO - Started process (PID=3629) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:29:17.633+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:29:17.636+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:29:17.635+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:29:17.786+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:29:17.809+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:29:17.808+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:29:17.823+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:29:17.823+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:29:17.840+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.213 seconds
[2025-01-10T17:29:47.949+0000] {processor.py:186} INFO - Started process (PID=3636) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:29:47.950+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:29:47.952+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:29:47.952+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:29:48.093+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:29:48.115+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:29:48.115+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:29:48.129+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:29:48.128+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:29:48.152+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T17:30:18.208+0000] {processor.py:186} INFO - Started process (PID=3643) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:30:18.209+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:30:18.211+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:30:18.210+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:30:18.350+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:30:18.374+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:30:18.373+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:30:18.387+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:30:18.386+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:30:18.403+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T17:30:48.514+0000] {processor.py:186} INFO - Started process (PID=3650) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:30:48.515+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:30:48.517+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:30:48.516+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:30:48.660+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:30:48.683+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:30:48.683+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:30:48.697+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:30:48.697+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:30:48.720+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.213 seconds
[2025-01-10T17:31:18.819+0000] {processor.py:186} INFO - Started process (PID=3657) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:31:18.821+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:31:18.823+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:31:18.823+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:31:18.988+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:31:19.015+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:31:19.015+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:31:19.031+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:31:19.030+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:31:19.047+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.234 seconds
[2025-01-10T17:31:49.156+0000] {processor.py:186} INFO - Started process (PID=3664) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:31:49.157+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:31:49.159+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:31:49.159+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:31:49.301+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:31:49.324+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:31:49.323+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:31:49.336+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:31:49.336+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:31:49.361+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.212 seconds
[2025-01-10T17:32:19.748+0000] {processor.py:186} INFO - Started process (PID=3671) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:32:19.749+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:32:19.751+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:32:19.751+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:32:19.893+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:32:19.918+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:32:19.918+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:32:19.933+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:32:19.933+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:32:19.959+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.217 seconds
[2025-01-10T17:32:50.064+0000] {processor.py:186} INFO - Started process (PID=3679) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:32:50.065+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:32:50.067+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:32:50.067+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:32:50.206+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:32:50.230+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:32:50.229+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:32:50.243+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:32:50.243+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:32:50.266+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T17:33:20.542+0000] {processor.py:186} INFO - Started process (PID=3686) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:33:20.543+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:33:20.545+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:33:20.545+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:33:20.689+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:33:20.711+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:33:20.711+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:33:20.726+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:33:20.726+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:33:20.742+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T17:33:50.933+0000] {processor.py:186} INFO - Started process (PID=3693) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:33:50.934+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:33:50.936+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:33:50.936+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:33:51.080+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:33:51.100+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:33:51.100+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:33:51.113+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:33:51.113+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:33:51.129+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T17:34:21.403+0000] {processor.py:186} INFO - Started process (PID=3700) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:34:21.404+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:34:21.406+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:34:21.406+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:34:21.548+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:34:21.571+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:34:21.571+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:34:21.583+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:34:21.583+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:34:21.599+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T17:34:51.825+0000] {processor.py:186} INFO - Started process (PID=3709) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:34:51.826+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:34:51.828+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:34:51.828+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:34:51.969+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:34:51.990+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:34:51.990+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:34:52.003+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:34:52.003+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:34:52.027+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T17:35:22.277+0000] {processor.py:186} INFO - Started process (PID=3716) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:35:22.278+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:35:22.280+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:35:22.280+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:35:22.423+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:35:22.446+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:35:22.446+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:35:22.462+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:35:22.461+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:35:22.478+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T17:35:52.588+0000] {processor.py:186} INFO - Started process (PID=3724) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:35:52.589+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:35:52.591+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:35:52.591+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:35:52.741+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:35:52.770+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:35:52.769+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:35:52.782+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:35:52.782+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:35:52.800+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.218 seconds
[2025-01-10T17:36:22.877+0000] {processor.py:186} INFO - Started process (PID=3731) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:36:22.878+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:36:22.880+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:36:22.880+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:36:23.027+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:36:23.050+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:36:23.049+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:36:23.063+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:36:23.063+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:36:23.081+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T17:36:53.202+0000] {processor.py:186} INFO - Started process (PID=3737) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:36:53.203+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:36:53.205+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:36:53.205+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:36:53.358+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:36:53.380+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:36:53.380+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:36:53.393+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:36:53.393+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:36:53.411+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.215 seconds
[2025-01-10T17:37:23.523+0000] {processor.py:186} INFO - Started process (PID=3744) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:37:23.524+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:37:23.527+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:37:23.526+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:37:23.684+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:37:23.706+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:37:23.705+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:37:23.723+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:37:23.723+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:37:23.739+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.222 seconds
[2025-01-10T17:37:54.082+0000] {processor.py:186} INFO - Started process (PID=3751) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:37:54.083+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:37:54.085+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:37:54.085+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:37:54.230+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:37:54.251+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:37:54.251+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:37:54.264+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:37:54.264+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:37:54.281+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T17:38:24.465+0000] {processor.py:186} INFO - Started process (PID=3758) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:38:24.465+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:38:24.468+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:38:24.467+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:38:24.613+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:38:24.637+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:38:24.637+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:38:24.650+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:38:24.650+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:38:24.666+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T17:38:55.060+0000] {processor.py:186} INFO - Started process (PID=3765) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:38:55.061+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:38:55.064+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:38:55.063+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:38:55.225+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:38:55.262+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:38:55.262+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:38:55.277+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:38:55.277+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:38:55.295+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.241 seconds
[2025-01-10T17:39:25.467+0000] {processor.py:186} INFO - Started process (PID=3771) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:39:25.467+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:39:25.470+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:39:25.469+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:39:25.608+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:39:25.631+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:39:25.630+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:39:25.644+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:39:25.643+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:39:25.667+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T17:39:55.832+0000] {processor.py:186} INFO - Started process (PID=3778) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:39:55.833+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:39:55.835+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:39:55.834+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:39:55.971+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:39:55.993+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:39:55.993+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:39:56.006+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:39:56.006+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:39:56.038+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.212 seconds
[2025-01-10T17:40:26.345+0000] {processor.py:186} INFO - Started process (PID=3785) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:40:26.346+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:40:26.348+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:40:26.347+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:40:26.488+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:40:26.512+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:40:26.512+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:40:26.526+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:40:26.525+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:40:26.548+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T17:40:56.707+0000] {processor.py:186} INFO - Started process (PID=3793) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:40:56.708+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:40:56.710+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:40:56.710+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:40:56.850+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:40:56.874+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:40:56.873+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:40:56.888+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:40:56.887+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:40:56.906+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T17:41:27.242+0000] {processor.py:186} INFO - Started process (PID=3800) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:41:27.243+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:41:27.245+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:41:27.245+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:41:27.385+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:41:27.406+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:41:27.406+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:41:27.419+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:41:27.419+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:41:27.440+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T17:41:57.609+0000] {processor.py:186} INFO - Started process (PID=3807) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:41:57.610+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:41:57.612+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:41:57.612+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:41:57.756+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:41:57.778+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:41:57.778+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:41:57.791+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:41:57.790+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:41:57.808+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T17:42:28.037+0000] {processor.py:186} INFO - Started process (PID=3814) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:42:28.037+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:42:28.040+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:42:28.039+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:42:28.180+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:42:28.203+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:42:28.202+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:42:28.216+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:42:28.216+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:42:28.233+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T17:42:58.568+0000] {processor.py:186} INFO - Started process (PID=3821) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:42:58.569+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:42:58.571+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:42:58.570+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:42:58.711+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:42:58.733+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:42:58.733+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:42:58.746+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:42:58.746+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:42:58.763+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T17:43:29.017+0000] {processor.py:186} INFO - Started process (PID=3828) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:43:29.018+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:43:29.021+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:43:29.020+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:43:29.169+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:43:29.190+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:43:29.190+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:43:29.203+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:43:29.203+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:43:29.220+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T17:43:59.421+0000] {processor.py:186} INFO - Started process (PID=3835) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:43:59.422+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:43:59.424+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:43:59.424+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:43:59.567+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:43:59.588+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:43:59.588+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:43:59.602+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:43:59.601+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:43:59.624+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T17:44:29.887+0000] {processor.py:186} INFO - Started process (PID=3842) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:44:29.888+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:44:29.890+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:44:29.890+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:44:30.030+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:44:30.052+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:44:30.051+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:44:30.065+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:44:30.065+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:44:30.087+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T17:45:00.183+0000] {processor.py:186} INFO - Started process (PID=3849) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:45:00.184+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:45:00.186+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:45:00.185+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:45:00.326+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:45:00.348+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:45:00.347+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:45:00.360+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:45:00.360+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:45:00.384+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T17:45:30.494+0000] {processor.py:186} INFO - Started process (PID=3856) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:45:30.495+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:45:30.497+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:45:30.497+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:45:30.651+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:45:30.677+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:45:30.676+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:45:30.702+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:45:30.701+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:45:30.719+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.230 seconds
[2025-01-10T17:46:00.781+0000] {processor.py:186} INFO - Started process (PID=3863) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:46:00.782+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:46:00.784+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:46:00.784+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:46:00.926+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:46:00.948+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:46:00.948+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:46:00.961+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:46:00.961+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:46:00.980+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T17:46:31.106+0000] {processor.py:186} INFO - Started process (PID=3870) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:46:31.107+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:46:31.110+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:46:31.109+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:46:31.269+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:46:31.294+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:46:31.294+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:46:31.307+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:46:31.307+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:46:31.322+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.222 seconds
[2025-01-10T17:47:01.414+0000] {processor.py:186} INFO - Started process (PID=3877) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:47:01.415+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:47:01.417+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:47:01.417+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:47:01.554+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:47:01.577+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:47:01.576+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:47:01.590+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:47:01.589+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:47:01.627+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.219 seconds
[2025-01-10T17:47:31.718+0000] {processor.py:186} INFO - Started process (PID=3884) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:47:31.719+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:47:31.721+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:47:31.721+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:47:31.864+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:47:31.887+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:47:31.886+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:47:31.900+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:47:31.900+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:47:31.918+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T17:48:02.037+0000] {processor.py:186} INFO - Started process (PID=3891) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:48:02.038+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:48:02.040+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:48:02.040+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:48:02.189+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:48:02.212+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:48:02.211+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:48:02.226+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:48:02.226+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:48:02.243+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.211 seconds
[2025-01-10T17:48:32.556+0000] {processor.py:186} INFO - Started process (PID=3898) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:48:32.557+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:48:32.560+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:48:32.560+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:48:32.700+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:48:32.721+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:48:32.721+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:48:32.737+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:48:32.737+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:48:32.752+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T17:49:02.944+0000] {processor.py:186} INFO - Started process (PID=3905) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:49:02.945+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:49:02.947+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:49:02.947+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:49:03.088+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:49:03.109+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:49:03.109+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:49:03.122+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:49:03.122+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:49:03.144+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T17:49:33.447+0000] {processor.py:186} INFO - Started process (PID=3912) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:49:33.448+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:49:33.450+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:49:33.450+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:49:33.591+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:49:33.612+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:49:33.612+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:49:33.625+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:49:33.625+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:49:33.649+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T17:50:03.841+0000] {processor.py:186} INFO - Started process (PID=3920) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:50:03.842+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:50:03.845+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:50:03.844+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:50:03.986+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:50:04.012+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:50:04.011+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:50:04.024+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:50:04.024+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:50:04.042+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.217 seconds
[2025-01-10T17:50:34.396+0000] {processor.py:186} INFO - Started process (PID=3928) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:50:34.397+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:50:34.400+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:50:34.399+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:50:34.540+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:50:34.563+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:50:34.562+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:50:34.575+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:50:34.575+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:50:34.591+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T17:51:04.723+0000] {processor.py:186} INFO - Started process (PID=3935) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:51:04.724+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:51:04.730+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:51:04.729+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:51:04.888+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:51:04.928+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:51:04.928+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:51:04.941+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:51:04.941+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:51:04.956+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.239 seconds
[2025-01-10T17:51:35.050+0000] {processor.py:186} INFO - Started process (PID=3943) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:51:35.051+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:51:35.053+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:51:35.053+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:51:35.193+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:51:35.216+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:51:35.215+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:51:35.229+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:51:35.229+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:51:35.245+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.201 seconds
[2025-01-10T17:52:05.656+0000] {processor.py:186} INFO - Started process (PID=3950) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:52:05.657+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:52:05.659+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:52:05.659+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:52:05.799+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:52:05.820+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:52:05.820+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:52:05.832+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:52:05.832+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:52:05.849+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.198 seconds
[2025-01-10T17:52:36.028+0000] {processor.py:186} INFO - Started process (PID=3956) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:52:36.029+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:52:36.031+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:52:36.031+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:52:36.173+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:52:36.195+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:52:36.195+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:52:36.208+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:52:36.208+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:52:36.231+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T17:53:06.605+0000] {processor.py:186} INFO - Started process (PID=3963) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:53:06.606+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:53:06.608+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:53:06.607+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:53:06.747+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:53:06.769+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:53:06.768+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:53:06.781+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:53:06.781+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:53:06.804+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T17:53:36.958+0000] {processor.py:186} INFO - Started process (PID=3970) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:53:36.971+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:53:36.974+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:53:36.973+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:53:37.195+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:53:37.215+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:53:37.215+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:53:37.229+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:53:37.228+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:53:37.251+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.299 seconds
[2025-01-10T17:54:07.462+0000] {processor.py:186} INFO - Started process (PID=3977) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:54:07.462+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:54:07.465+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:54:07.464+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:54:07.626+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:54:07.650+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:54:07.650+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:54:07.665+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:54:07.665+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:54:07.683+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.228 seconds
[2025-01-10T17:54:38.046+0000] {processor.py:186} INFO - Started process (PID=3984) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:54:38.047+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:54:38.050+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:54:38.049+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:54:38.191+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:54:38.214+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:54:38.213+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:54:38.227+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:54:38.226+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:54:38.249+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T17:55:08.367+0000] {processor.py:186} INFO - Started process (PID=3992) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:55:08.368+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:55:08.370+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:55:08.370+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:55:08.511+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:55:08.534+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:55:08.533+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:55:08.547+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:55:08.547+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:55:08.565+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T17:55:38.712+0000] {processor.py:186} INFO - Started process (PID=3999) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:55:38.713+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:55:38.715+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:55:38.714+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:55:38.855+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:55:38.877+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:55:38.876+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:55:38.889+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:55:38.889+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:55:38.922+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.216 seconds
[2025-01-10T17:56:09.019+0000] {processor.py:186} INFO - Started process (PID=4006) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:56:09.020+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:56:09.022+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:56:09.021+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:56:09.162+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:56:09.183+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:56:09.183+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:56:09.196+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:56:09.196+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:56:09.216+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.203 seconds
[2025-01-10T17:56:39.351+0000] {processor.py:186} INFO - Started process (PID=4014) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:56:39.352+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:56:39.354+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:56:39.354+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:56:39.493+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:56:39.517+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:56:39.516+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:56:39.529+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:56:39.529+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:56:39.546+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T17:57:09.647+0000] {processor.py:186} INFO - Started process (PID=4021) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:57:09.647+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:57:09.650+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:57:09.649+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:57:09.793+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:57:09.816+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:57:09.815+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:57:09.828+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:57:09.828+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:57:09.852+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.212 seconds
[2025-01-10T17:57:39.946+0000] {processor.py:186} INFO - Started process (PID=4027) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:57:39.947+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:57:39.949+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:57:39.949+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:57:40.093+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:57:40.116+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:57:40.115+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:57:40.129+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:57:40.129+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:57:40.147+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T17:58:10.231+0000] {processor.py:186} INFO - Started process (PID=4034) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:58:10.232+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:58:10.234+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:58:10.234+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:58:10.372+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:58:10.394+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:58:10.394+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:58:10.409+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:58:10.409+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:58:10.434+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T17:58:40.564+0000] {processor.py:186} INFO - Started process (PID=4041) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:58:40.565+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:58:40.568+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:58:40.567+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:58:40.711+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:58:40.734+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:58:40.734+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:58:40.747+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:58:40.747+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:58:40.771+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.213 seconds
[2025-01-10T17:59:10.910+0000] {processor.py:186} INFO - Started process (PID=4048) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:59:10.911+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:59:10.913+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:59:10.913+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:59:11.052+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:59:11.073+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:59:11.073+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:59:11.086+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:59:11.086+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:59:11.104+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T17:59:41.206+0000] {processor.py:186} INFO - Started process (PID=4055) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T17:59:41.207+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T17:59:41.209+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:59:41.209+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:59:41.352+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T17:59:41.374+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:59:41.374+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T17:59:41.392+0000] {logging_mixin.py:190} INFO - [2025-01-10T17:59:41.392+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T17:59:41.421+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.221 seconds
[2025-01-10T18:00:11.546+0000] {processor.py:186} INFO - Started process (PID=4063) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:00:11.546+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:00:11.549+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:00:11.548+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:00:11.692+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:00:11.714+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:00:11.714+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:00:11.730+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:00:11.730+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:00:11.748+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T18:00:42.033+0000] {processor.py:186} INFO - Started process (PID=4070) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:00:42.034+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:00:42.037+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:00:42.036+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:00:42.359+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:00:42.395+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:00:42.395+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:00:42.425+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:00:42.424+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:00:42.465+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.448 seconds
[2025-01-10T18:01:12.876+0000] {processor.py:186} INFO - Started process (PID=4077) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:01:12.877+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:01:12.879+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:01:12.879+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:01:13.064+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:01:13.089+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:01:13.088+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:01:13.104+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:01:13.104+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:01:13.124+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.255 seconds
[2025-01-10T18:01:43.501+0000] {processor.py:186} INFO - Started process (PID=4083) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:01:43.502+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:01:43.504+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:01:43.504+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:01:43.651+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:01:43.674+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:01:43.674+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:01:43.690+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:01:43.689+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:01:43.706+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.210 seconds
[2025-01-10T18:02:13.845+0000] {processor.py:186} INFO - Started process (PID=4090) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:02:13.845+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:02:13.847+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:02:13.847+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:02:13.986+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:02:14.008+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:02:14.007+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:02:14.020+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:02:14.020+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:02:14.039+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T18:02:44.327+0000] {processor.py:186} INFO - Started process (PID=4097) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:02:44.328+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:02:44.330+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:02:44.330+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:02:44.471+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:02:44.493+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:02:44.492+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:02:44.506+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:02:44.506+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:02:44.529+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T18:03:14.711+0000] {processor.py:186} INFO - Started process (PID=4103) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:03:14.712+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:03:14.714+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:03:14.714+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:03:14.856+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:03:14.878+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:03:14.878+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:03:14.891+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:03:14.891+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:03:14.915+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T18:03:45.223+0000] {processor.py:186} INFO - Started process (PID=4110) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:03:45.224+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:03:45.229+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:03:45.229+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:03:45.433+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:03:45.455+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:03:45.454+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:03:45.467+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:03:45.467+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:03:45.484+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.267 seconds
[2025-01-10T18:04:15.552+0000] {processor.py:186} INFO - Started process (PID=4124) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:04:15.553+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:04:15.555+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:04:15.555+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:04:15.695+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:04:15.716+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:04:15.716+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:04:15.729+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:04:15.729+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:04:15.745+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.199 seconds
[2025-01-10T18:04:45.942+0000] {processor.py:186} INFO - Started process (PID=4131) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:04:45.943+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:04:45.945+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:04:45.945+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:04:46.085+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:04:46.107+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:04:46.107+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:04:46.120+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:04:46.120+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:04:46.142+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T18:05:16.388+0000] {processor.py:186} INFO - Started process (PID=4138) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:05:16.389+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:05:16.391+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:05:16.391+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:05:16.536+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:05:16.557+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:05:16.556+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:05:16.571+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:05:16.571+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:05:16.594+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.212 seconds
[2025-01-10T18:05:46.745+0000] {processor.py:186} INFO - Started process (PID=4145) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:05:46.746+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:05:46.748+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:05:46.748+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:05:46.887+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:05:46.909+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:05:46.909+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:05:46.922+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:05:46.922+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:05:46.939+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.199 seconds
[2025-01-10T18:06:17.266+0000] {processor.py:186} INFO - Started process (PID=4153) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:06:17.267+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:06:17.269+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:06:17.269+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:06:17.412+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:06:17.433+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:06:17.433+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:06:17.446+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:06:17.446+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:06:17.469+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.208 seconds
[2025-01-10T18:06:47.656+0000] {processor.py:186} INFO - Started process (PID=4160) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:06:47.657+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:06:47.659+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:06:47.658+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:06:47.804+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:06:47.826+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:06:47.825+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:06:47.838+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:06:47.838+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:06:47.861+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.211 seconds
[2025-01-10T18:07:18.130+0000] {processor.py:186} INFO - Started process (PID=4167) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:07:18.131+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:07:18.133+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:07:18.133+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:07:18.275+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:07:18.299+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:07:18.298+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:07:18.311+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:07:18.311+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:07:18.335+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.210 seconds
[2025-01-10T18:07:48.462+0000] {processor.py:186} INFO - Started process (PID=4174) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:07:48.463+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:07:48.465+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:07:48.464+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:07:48.603+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:07:48.626+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:07:48.626+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:07:48.639+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:07:48.639+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:07:48.663+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T18:08:19.012+0000] {processor.py:186} INFO - Started process (PID=4181) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:08:19.018+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:08:19.021+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:08:19.021+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:08:19.191+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:08:19.213+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:08:19.213+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:08:19.228+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:08:19.228+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:08:19.247+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.242 seconds
[2025-01-10T18:08:49.330+0000] {processor.py:186} INFO - Started process (PID=4189) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:08:49.331+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:08:49.333+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:08:49.333+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:08:49.475+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:08:49.498+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:08:49.498+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:08:49.511+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:08:49.511+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:08:49.528+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T18:09:19.692+0000] {processor.py:186} INFO - Started process (PID=4196) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:09:19.693+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:09:19.701+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:09:19.700+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:09:19.879+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:09:19.905+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:09:19.905+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:09:19.919+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:09:19.919+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:09:19.937+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.250 seconds
[2025-01-10T18:09:50.286+0000] {processor.py:186} INFO - Started process (PID=4203) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:09:50.286+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:09:50.289+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:09:50.288+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:09:50.427+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:09:50.449+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:09:50.448+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:09:50.461+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:09:50.461+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:09:50.484+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T18:10:20.642+0000] {processor.py:186} INFO - Started process (PID=4211) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:10:20.642+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:10:20.645+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:10:20.644+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:10:20.782+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:10:20.803+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:10:20.803+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:10:20.816+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:10:20.816+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:10:20.840+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T18:10:51.195+0000] {processor.py:186} INFO - Started process (PID=4218) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:10:51.195+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:10:51.198+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:10:51.197+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:10:51.338+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:10:51.360+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:10:51.359+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:10:51.372+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:10:51.372+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:10:51.411+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.222 seconds
[2025-01-10T18:11:21.558+0000] {processor.py:186} INFO - Started process (PID=4226) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:11:21.559+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:11:21.561+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:11:21.561+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:11:21.701+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:11:21.724+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:11:21.723+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:11:21.739+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:11:21.739+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:11:21.777+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.225 seconds
[2025-01-10T18:11:52.129+0000] {processor.py:186} INFO - Started process (PID=4234) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:11:52.130+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:11:52.132+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:11:52.132+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:11:52.273+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:11:52.295+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:11:52.294+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:11:52.307+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:11:52.307+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:11:52.324+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T18:12:22.507+0000] {processor.py:186} INFO - Started process (PID=4241) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:12:22.508+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:12:22.510+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:12:22.510+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:12:22.652+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:12:22.675+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:12:22.675+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:12:22.691+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:12:22.691+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:12:22.713+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.212 seconds
[2025-01-10T18:12:53.054+0000] {processor.py:186} INFO - Started process (PID=4248) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:12:53.055+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:12:53.057+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:12:53.057+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:12:53.196+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:12:53.218+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:12:53.218+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:12:53.231+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:12:53.231+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:12:53.253+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.204 seconds
[2025-01-10T18:13:23.401+0000] {processor.py:186} INFO - Started process (PID=4255) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:13:23.401+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:13:23.404+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:13:23.403+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:13:23.567+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:13:23.591+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:13:23.591+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:13:23.605+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:13:23.605+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:13:23.622+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.227 seconds
[2025-01-10T18:13:53.875+0000] {processor.py:186} INFO - Started process (PID=4262) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:13:53.876+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:13:53.878+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:13:53.878+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:13:54.017+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:13:54.039+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:13:54.038+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:13:54.051+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:13:54.051+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:13:54.075+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.206 seconds
[2025-01-10T18:14:24.200+0000] {processor.py:186} INFO - Started process (PID=4270) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:14:24.201+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:14:24.203+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:14:24.203+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:14:24.342+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:14:24.364+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:14:24.363+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:14:24.377+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:14:24.377+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:14:24.401+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T18:14:54.755+0000] {processor.py:186} INFO - Started process (PID=4279) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:14:54.756+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:14:54.758+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:14:54.758+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:14:54.915+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:14:54.937+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:14:54.937+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:14:54.949+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:14:54.949+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:14:54.983+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.232 seconds
[2025-01-10T18:15:25.059+0000] {processor.py:186} INFO - Started process (PID=4286) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:15:25.059+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:15:25.062+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:15:25.061+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:15:25.208+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:15:25.231+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:15:25.231+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:15:25.247+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:15:25.247+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:15:25.267+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.214 seconds
[2025-01-10T18:15:55.611+0000] {processor.py:186} INFO - Started process (PID=4293) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:15:55.612+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:15:55.615+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:15:55.614+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:15:55.768+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:15:55.791+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:15:55.791+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:15:55.805+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:15:55.805+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:15:55.828+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.221 seconds
[2025-01-10T18:16:25.996+0000] {processor.py:186} INFO - Started process (PID=4300) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:16:25.998+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:16:26.005+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:16:26.005+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:16:26.157+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:16:26.178+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:16:26.177+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:16:26.190+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:16:26.190+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:16:26.210+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.228 seconds
[2025-01-10T18:16:56.570+0000] {processor.py:186} INFO - Started process (PID=4307) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:16:56.571+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:16:56.573+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:16:56.573+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:16:56.715+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:16:56.738+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:16:56.737+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:16:56.751+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:16:56.751+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:16:56.774+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.210 seconds
[2025-01-10T18:17:26.930+0000] {processor.py:186} INFO - Started process (PID=4314) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:17:26.931+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:17:26.933+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:17:26.933+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:17:27.073+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:17:27.095+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:17:27.094+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:17:27.107+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:17:27.107+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:17:27.125+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T18:17:57.448+0000] {processor.py:186} INFO - Started process (PID=4322) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:17:57.449+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:17:57.451+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:17:57.451+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:17:57.611+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:17:57.634+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:17:57.633+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:17:57.648+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:17:57.648+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:17:57.665+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.223 seconds
[2025-01-10T18:18:27.829+0000] {processor.py:186} INFO - Started process (PID=4329) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:18:27.830+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:18:27.832+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:18:27.832+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:18:27.971+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:18:27.993+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:18:27.992+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:18:28.006+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:18:28.006+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:18:28.028+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.205 seconds
[2025-01-10T18:18:58.330+0000] {processor.py:186} INFO - Started process (PID=4336) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:18:58.331+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:18:58.333+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:18:58.332+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:18:58.473+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:18:58.497+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:18:58.497+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:18:58.510+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:18:58.510+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:18:58.527+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T18:19:28.682+0000] {processor.py:186} INFO - Started process (PID=4343) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:19:28.683+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:19:28.685+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:19:28.685+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:19:28.825+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:19:28.848+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:19:28.848+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:19:28.861+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:19:28.861+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:19:28.878+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.202 seconds
[2025-01-10T18:19:59.266+0000] {processor.py:186} INFO - Started process (PID=4350) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:19:59.267+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:19:59.269+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:19:59.268+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:19:59.406+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:19:59.426+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:19:59.426+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:19:59.439+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:19:59.439+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:19:59.460+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.199 seconds
[2025-01-10T18:20:29.603+0000] {processor.py:186} INFO - Started process (PID=4358) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:20:29.603+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:20:29.606+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:20:29.605+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:20:29.745+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:20:29.767+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:20:29.767+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:20:29.780+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:20:29.780+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:20:29.804+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T18:21:00.134+0000] {processor.py:186} INFO - Started process (PID=4365) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:21:00.135+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:21:00.137+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:21:00.136+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:21:00.276+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:21:00.297+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:21:00.296+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:21:00.310+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:21:00.310+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:21:00.349+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.220 seconds
[2025-01-10T18:21:30.539+0000] {processor.py:186} INFO - Started process (PID=4372) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:21:30.540+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:21:30.542+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:21:30.542+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:21:30.682+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:21:30.702+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:21:30.702+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:21:30.715+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:21:30.715+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:21:30.732+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T18:22:00.977+0000] {processor.py:186} INFO - Started process (PID=4379) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:22:00.978+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:22:00.980+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:22:00.980+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:22:01.121+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:22:01.146+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:22:01.145+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:22:01.158+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:22:01.158+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:22:01.180+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.209 seconds
[2025-01-10T18:22:31.260+0000] {processor.py:186} INFO - Started process (PID=4386) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:22:31.261+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:22:31.263+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:22:31.263+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:22:31.401+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:22:31.424+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:22:31.423+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:22:31.437+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:22:31.436+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:22:31.461+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T18:23:01.641+0000] {processor.py:186} INFO - Started process (PID=4393) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:23:01.642+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:23:01.645+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:23:01.644+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:23:01.782+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:23:01.805+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:23:01.804+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:23:01.818+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:23:01.818+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:23:01.835+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.200 seconds
[2025-01-10T18:23:31.982+0000] {processor.py:186} INFO - Started process (PID=4400) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:23:31.983+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:23:31.986+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:23:31.985+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:23:32.146+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:23:32.174+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:23:32.173+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:23:32.188+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:23:32.188+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:23:32.204+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.228 seconds
[2025-01-10T18:24:02.590+0000] {processor.py:186} INFO - Started process (PID=4407) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:24:02.591+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:24:02.593+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:24:02.593+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:24:02.737+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:24:02.758+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:24:02.758+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:24:02.777+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:24:02.777+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:24:02.801+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.217 seconds
[2025-01-10T18:24:32.886+0000] {processor.py:186} INFO - Started process (PID=4413) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:24:32.887+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:24:32.889+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:24:32.889+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:24:33.035+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:24:33.063+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:24:33.063+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:24:33.080+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:24:33.080+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:24:33.107+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.227 seconds
[2025-01-10T18:25:03.335+0000] {processor.py:186} INFO - Started process (PID=4421) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:25:03.337+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:25:03.340+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:25:03.339+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:25:03.534+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:25:03.563+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:25:03.563+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:25:03.583+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:25:03.583+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:25:03.612+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.285 seconds
[2025-01-10T18:25:33.905+0000] {processor.py:186} INFO - Started process (PID=4428) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:25:33.906+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:25:33.908+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:25:33.908+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:25:34.066+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:25:34.093+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:25:34.092+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:25:34.109+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:25:34.109+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:25:34.132+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.233 seconds
[2025-01-10T18:26:04.354+0000] {processor.py:186} INFO - Started process (PID=4435) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:26:04.355+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:26:04.358+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:26:04.357+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:26:04.540+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:26:04.565+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:26:04.564+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:26:04.581+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:26:04.580+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:26:04.606+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.259 seconds
[2025-01-10T18:26:34.868+0000] {processor.py:186} INFO - Started process (PID=4442) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:26:34.868+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:26:34.871+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:26:34.870+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:26:35.027+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:26:35.050+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:26:35.049+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:26:35.064+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:26:35.063+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:26:35.155+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.293 seconds
[2025-01-10T18:27:05.434+0000] {processor.py:186} INFO - Started process (PID=4449) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:27:05.436+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:27:05.441+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:27:05.440+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:27:05.604+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:27:05.633+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:27:05.633+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:27:05.657+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:27:05.657+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:27:05.681+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.254 seconds
[2025-01-10T18:27:35.916+0000] {processor.py:186} INFO - Started process (PID=4456) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:27:35.917+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:27:35.920+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:27:35.919+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:27:36.086+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:27:36.109+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:27:36.108+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:27:36.124+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:27:36.124+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:27:36.142+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.232 seconds
[2025-01-10T18:28:06.325+0000] {processor.py:186} INFO - Started process (PID=4463) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:28:06.327+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:28:06.332+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:28:06.330+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:28:06.497+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:28:06.520+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:28:06.520+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:28:06.534+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:28:06.534+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:28:06.551+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.233 seconds
[2025-01-10T18:28:36.804+0000] {processor.py:186} INFO - Started process (PID=4470) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:28:36.805+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:28:36.807+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:28:36.807+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:28:36.996+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:28:37.018+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:28:37.018+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:28:37.036+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:28:37.036+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:28:37.053+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.255 seconds
[2025-01-10T18:29:07.285+0000] {processor.py:186} INFO - Started process (PID=4477) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:29:07.286+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:29:07.289+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:29:07.288+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:29:07.449+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:29:07.480+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:29:07.479+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:29:07.499+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:29:07.499+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:29:07.529+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.250 seconds
[2025-01-10T18:29:37.832+0000] {processor.py:186} INFO - Started process (PID=4484) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:29:37.834+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:29:37.840+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:29:37.839+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:29:38.048+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:29:38.102+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:29:38.102+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:29:38.130+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:29:38.130+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:29:38.156+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.337 seconds
[2025-01-10T18:30:08.406+0000] {processor.py:186} INFO - Started process (PID=4491) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:30:08.407+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:30:08.409+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:30:08.409+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:30:08.559+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:30:08.590+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:30:08.589+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:30:08.610+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:30:08.610+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:30:08.637+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.237 seconds
[2025-01-10T18:30:38.924+0000] {processor.py:186} INFO - Started process (PID=4498) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:30:38.925+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:30:38.927+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:30:38.927+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:30:39.087+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:30:39.112+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:30:39.112+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:30:39.128+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:30:39.128+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:30:39.161+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.243 seconds
[2025-01-10T18:31:09.419+0000] {processor.py:186} INFO - Started process (PID=4506) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:31:09.420+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:31:09.422+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:31:09.421+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:31:09.565+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:31:09.589+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:31:09.589+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:31:09.603+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:31:09.603+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:31:09.620+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.207 seconds
[2025-01-10T18:31:39.691+0000] {processor.py:186} INFO - Started process (PID=4513) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:31:39.692+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:31:39.694+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:31:39.694+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:31:39.854+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:31:39.878+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:31:39.878+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:31:39.891+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:31:39.891+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:31:39.914+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.229 seconds
[2025-01-10T18:32:10.098+0000] {processor.py:186} INFO - Started process (PID=4520) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:32:10.099+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:32:10.102+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:32:10.101+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:32:10.256+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:32:10.280+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:32:10.279+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:32:10.295+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:32:10.295+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:32:10.323+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.231 seconds
[2025-01-10T18:32:40.517+0000] {processor.py:186} INFO - Started process (PID=4527) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:32:40.518+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:32:40.520+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:32:40.520+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:32:40.674+0000] {processor.py:925} INFO - DAG(s) 'scrape_and_insert_jobs' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:32:40.697+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:32:40.696+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:32:40.710+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:32:40.709+0000] {dag.py:4180} INFO - Setting next_dagrun for scrape_and_insert_jobs to None, run_after=None
[2025-01-10T18:32:40.728+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.216 seconds
[2025-01-10T18:32:50.891+0000] {processor.py:186} INFO - Started process (PID=4534) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:32:50.892+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:32:50.894+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:32:50.894+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:32:51.063+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:32:51.054+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/crawl_data.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 999, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/crawl_data.py", line 226, in <module>
    python_callable=insert_jobs,
                    ^^^^^^^^^^^
NameError: name 'insert_jobs' is not defined
[2025-01-10T18:32:51.070+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:32:51.094+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.210 seconds
[2025-01-10T18:33:21.343+0000] {processor.py:186} INFO - Started process (PID=4540) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:33:21.344+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:33:21.346+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:33:21.346+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:33:21.497+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:33:21.488+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/crawl_data.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 999, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/crawl_data.py", line 226, in <module>
    python_callable=insert_jobs,
                    ^^^^^^^^^^^
NameError: name 'insert_jobs' is not defined
[2025-01-10T18:33:21.504+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:33:21.526+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.189 seconds
[2025-01-10T18:33:44.550+0000] {processor.py:186} INFO - Started process (PID=4541) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:33:44.551+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:33:44.554+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:33:44.553+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:33:44.735+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:33:44.727+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/crawl_data.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 999, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/crawl_data.py", line 209, in <module>
    'retry_delay': timedelta(minutes=5),
                   ^^^^^^^^^
NameError: name 'timedelta' is not defined
[2025-01-10T18:33:44.740+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:33:44.757+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.213 seconds
[2025-01-10T18:34:14.960+0000] {processor.py:186} INFO - Started process (PID=4549) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:34:14.961+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:34:14.963+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:34:14.962+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:34:15.164+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:34:15.154+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/crawl_data.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 999, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/crawl_data.py", line 209, in <module>
    'retry_delay': timedelta(minutes=5),
                   ^^^^^^^^^
NameError: name 'timedelta' is not defined
[2025-01-10T18:34:15.169+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:34:15.190+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.236 seconds
[2025-01-10T18:34:33.139+0000] {processor.py:186} INFO - Started process (PID=4556) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:34:33.140+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:34:33.141+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:34:33.140+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:34:33.311+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:34:33.425+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:34:33.424+0000] {override.py:1911} INFO - Created Permission View: can delete on DAG:topcv_scraper
[2025-01-10T18:34:33.434+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:34:33.433+0000] {override.py:1911} INFO - Created Permission View: can read on DAG:topcv_scraper
[2025-01-10T18:34:33.440+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:34:33.439+0000] {override.py:1911} INFO - Created Permission View: can edit on DAG:topcv_scraper
[2025-01-10T18:34:33.445+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:34:33.445+0000] {override.py:1911} INFO - Created Permission View: menu access on DAG Run:topcv_scraper
[2025-01-10T18:34:33.451+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:34:33.450+0000] {override.py:1911} INFO - Created Permission View: can delete on DAG Run:topcv_scraper
[2025-01-10T18:34:33.456+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:34:33.456+0000] {override.py:1911} INFO - Created Permission View: can create on DAG Run:topcv_scraper
[2025-01-10T18:34:33.461+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:34:33.461+0000] {override.py:1911} INFO - Created Permission View: can read on DAG Run:topcv_scraper
[2025-01-10T18:34:33.462+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:34:33.461+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:34:33.474+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:34:33.474+0000] {dag.py:3262} INFO - Creating ORM DAG for topcv_scraper
[2025-01-10T18:34:33.487+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:34:33.487+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-09 00:00:00+00:00, run_after=2025-01-10 00:00:00+00:00
[2025-01-10T18:34:33.514+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.381 seconds
[2025-01-10T18:35:03.571+0000] {processor.py:186} INFO - Started process (PID=4563) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:35:03.572+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:35:03.574+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:35:03.573+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:35:03.777+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:35:03.802+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:35:03.801+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:35:03.825+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:35:03.825+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:35:03.843+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.279 seconds
[2025-01-10T18:35:33.951+0000] {processor.py:186} INFO - Started process (PID=4570) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:35:33.952+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:35:33.954+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:35:33.953+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:35:34.123+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:35:34.148+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:35:34.147+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:35:34.173+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:35:34.172+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:35:34.189+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.245 seconds
[2025-01-10T18:36:04.282+0000] {processor.py:186} INFO - Started process (PID=4577) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:36:04.283+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:36:04.285+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:36:04.285+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:36:04.497+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:36:04.532+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:36:04.531+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:36:04.573+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:36:04.572+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:36:04.600+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.327 seconds
[2025-01-10T18:36:34.662+0000] {processor.py:186} INFO - Started process (PID=4584) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:36:34.663+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:36:34.665+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:36:34.664+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:36:34.828+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:36:34.853+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:36:34.853+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:36:34.878+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:36:34.878+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:36:34.902+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.246 seconds
[2025-01-10T18:37:05.156+0000] {processor.py:186} INFO - Started process (PID=4591) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:37:05.157+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:37:05.158+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:37:05.158+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:37:05.322+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:37:05.344+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:37:05.344+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:37:05.365+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:37:05.365+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:37:05.382+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.232 seconds
[2025-01-10T18:37:35.508+0000] {processor.py:186} INFO - Started process (PID=4598) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:37:35.508+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:37:35.510+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:37:35.509+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:37:35.656+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:37:35.678+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:37:35.678+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:37:35.699+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:37:35.699+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:37:35.718+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.216 seconds
[2025-01-10T18:38:06.085+0000] {processor.py:186} INFO - Started process (PID=4606) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:38:06.086+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:38:06.088+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:38:06.087+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:38:06.249+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:38:06.275+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:38:06.275+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:38:06.301+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:38:06.301+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:38:06.321+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.241 seconds
[2025-01-10T18:38:36.635+0000] {processor.py:186} INFO - Started process (PID=4613) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:38:36.635+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:38:36.637+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:38:36.636+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:38:36.780+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:38:36.803+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:38:36.802+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:38:36.833+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:38:36.832+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:38:36.849+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.220 seconds
[2025-01-10T18:39:06.916+0000] {processor.py:186} INFO - Started process (PID=4621) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:39:06.916+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:39:06.918+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:39:06.917+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:39:07.064+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:39:07.088+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:39:07.087+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:39:07.109+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:39:07.109+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:39:07.128+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.218 seconds
[2025-01-10T18:39:37.351+0000] {processor.py:186} INFO - Started process (PID=4628) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:39:37.352+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:39:37.353+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:39:37.353+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:39:37.521+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:39:37.544+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:39:37.544+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:39:37.569+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:39:37.568+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:39:37.584+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.240 seconds
[2025-01-10T18:40:07.662+0000] {processor.py:186} INFO - Started process (PID=4635) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:40:07.663+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:40:07.664+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:40:07.664+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:40:07.819+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:40:07.853+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:40:07.853+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:40:07.876+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:40:07.876+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:40:07.892+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.236 seconds
[2025-01-10T18:40:34.998+0000] {processor.py:186} INFO - Started process (PID=4643) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:40:34.999+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:40:35.000+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:40:35.000+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:40:35.345+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:40:35.382+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:40:35.382+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:40:35.415+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:40:35.415+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:40:35.455+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.465 seconds
[2025-01-10T18:40:56.650+0000] {processor.py:186} INFO - Started process (PID=4650) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:40:56.651+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:40:56.652+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:40:56.652+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:40:56.817+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:40:56.842+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:40:56.841+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:40:56.865+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:40:56.865+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:40:56.884+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.240 seconds
[2025-01-10T18:44:57.805+0000] {processor.py:186} INFO - Started process (PID=61) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:44:57.811+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:44:57.821+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:44:57.820+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:44:58.481+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:45:00.022+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:45:00.021+0000] {override.py:1911} INFO - Created Permission View: can edit on DAG:topcv_scraper
[2025-01-10T18:45:00.060+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:45:00.059+0000] {override.py:1911} INFO - Created Permission View: can delete on DAG:topcv_scraper
[2025-01-10T18:45:00.077+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:45:00.077+0000] {override.py:1911} INFO - Created Permission View: can read on DAG:topcv_scraper
[2025-01-10T18:45:00.097+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:45:00.097+0000] {override.py:1911} INFO - Created Permission View: can create on DAG Run:topcv_scraper
[2025-01-10T18:45:00.110+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:45:00.109+0000] {override.py:1911} INFO - Created Permission View: menu access on DAG Run:topcv_scraper
[2025-01-10T18:45:00.127+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:45:00.127+0000] {override.py:1911} INFO - Created Permission View: can delete on DAG Run:topcv_scraper
[2025-01-10T18:45:00.138+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:45:00.138+0000] {override.py:1911} INFO - Created Permission View: can read on DAG Run:topcv_scraper
[2025-01-10T18:45:00.139+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:45:00.138+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:45:00.183+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:45:00.177+0000] {dag.py:3262} INFO - Creating ORM DAG for topcv_scraper
[2025-01-10T18:45:00.210+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:45:00.210+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-09 00:00:00+00:00, run_after=2025-01-10 00:00:00+00:00
[2025-01-10T18:45:00.249+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 2.460 seconds
[2025-01-10T18:45:30.667+0000] {processor.py:186} INFO - Started process (PID=74) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:45:30.669+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:45:30.675+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:45:30.674+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:45:30.977+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:45:31.018+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:45:31.017+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:45:31.334+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:45:31.334+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-09 00:00:00+00:00, run_after=2025-01-10 00:00:00+00:00
[2025-01-10T18:45:31.367+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.717 seconds
[2025-01-10T18:46:01.441+0000] {processor.py:186} INFO - Started process (PID=81) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:46:01.442+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:46:01.446+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:46:01.446+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:46:01.709+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:46:01.751+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:46:01.750+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:46:02.006+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:46:02.005+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:46:02.044+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.615 seconds
[2025-01-10T18:46:32.320+0000] {processor.py:186} INFO - Started process (PID=87) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:46:32.322+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:46:32.326+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:46:32.325+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:46:32.817+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:46:32.854+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:46:32.853+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:46:32.889+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:46:32.889+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:46:32.923+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.613 seconds
[2025-01-10T18:47:03.200+0000] {processor.py:186} INFO - Started process (PID=94) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:47:03.202+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:47:03.206+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:47:03.205+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:47:03.693+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:47:03.727+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:47:03.726+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:47:03.763+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:47:03.763+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:47:03.791+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.603 seconds
[2025-01-10T18:47:34.123+0000] {processor.py:186} INFO - Started process (PID=100) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:47:34.129+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:47:34.133+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:47:34.132+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:47:34.699+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:47:34.743+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:47:34.742+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:47:34.803+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:47:34.802+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:47:34.846+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.734 seconds
[2025-01-10T18:48:05.149+0000] {processor.py:186} INFO - Started process (PID=106) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:48:05.150+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:48:05.154+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:48:05.154+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:48:06.022+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:48:06.077+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:48:06.076+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:48:06.177+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:48:06.177+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:48:06.202+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 1.085 seconds
[2025-01-10T18:48:36.392+0000] {processor.py:186} INFO - Started process (PID=113) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:48:36.394+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:48:36.399+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:48:36.397+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:48:37.078+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:48:37.127+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:48:37.126+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:48:37.173+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:48:37.173+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:48:37.216+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.836 seconds
[2025-01-10T18:49:07.595+0000] {processor.py:186} INFO - Started process (PID=120) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:49:07.597+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:49:07.601+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:49:07.601+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:49:08.176+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:49:08.215+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:49:08.214+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:49:08.263+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:49:08.262+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:49:08.295+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.710 seconds
[2025-01-10T18:49:38.680+0000] {processor.py:186} INFO - Started process (PID=127) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:49:38.681+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:49:38.687+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:49:38.686+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:49:39.260+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:49:39.314+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:49:39.313+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:49:39.370+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:49:39.369+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:49:39.413+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.745 seconds
[2025-01-10T18:50:09.826+0000] {processor.py:186} INFO - Started process (PID=134) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:50:09.828+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:50:09.834+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:50:09.834+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:50:10.377+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:50:10.417+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:50:10.416+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:50:10.465+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:50:10.465+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:50:10.505+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.691 seconds
[2025-01-10T18:50:40.766+0000] {processor.py:186} INFO - Started process (PID=141) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:50:40.769+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:50:40.776+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:50:40.775+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:50:41.628+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:50:41.678+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:50:41.677+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:50:41.730+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:50:41.729+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:50:41.771+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 1.031 seconds
[2025-01-10T18:51:12.034+0000] {processor.py:186} INFO - Started process (PID=149) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:51:12.035+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:51:12.048+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:51:12.041+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:51:12.684+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:51:12.725+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:51:12.724+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:51:12.767+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:51:12.767+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:51:12.830+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.809 seconds
[2025-01-10T18:51:43.180+0000] {processor.py:186} INFO - Started process (PID=156) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:51:43.198+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:51:43.202+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:51:43.201+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:51:44.248+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:51:44.324+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:51:44.324+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:51:44.422+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:51:44.421+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:51:44.498+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 1.342 seconds
[2025-01-10T18:52:14.711+0000] {processor.py:186} INFO - Started process (PID=163) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:52:14.713+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:52:14.718+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:52:14.717+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:52:15.287+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:52:15.328+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:52:15.327+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:52:15.377+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:52:15.377+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:52:15.412+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.713 seconds
[2025-01-10T18:52:45.816+0000] {processor.py:186} INFO - Started process (PID=171) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:52:45.817+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:52:45.822+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:52:45.821+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:52:46.378+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:52:46.408+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:52:46.407+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:52:46.439+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:52:46.438+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:52:46.467+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.665 seconds
[2025-01-10T18:53:16.783+0000] {processor.py:186} INFO - Started process (PID=178) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:53:16.785+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:53:16.789+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:53:16.788+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:53:17.308+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:53:17.344+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:53:17.343+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:53:17.384+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:53:17.384+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:53:17.423+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.650 seconds
[2025-01-10T18:53:47.613+0000] {processor.py:186} INFO - Started process (PID=185) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:53:47.614+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:53:47.618+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:53:47.618+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:53:48.095+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:53:48.129+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:53:48.127+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:53:48.176+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:53:48.176+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:53:48.207+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.603 seconds
[2025-01-10T18:54:18.527+0000] {processor.py:186} INFO - Started process (PID=192) to work on /opt/airflow/dags/crawl_data.py
[2025-01-10T18:54:18.530+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/crawl_data.py for tasks to queue
[2025-01-10T18:54:18.536+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:54:18.535+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:54:19.140+0000] {processor.py:925} INFO - DAG(s) 'topcv_scraper' retrieved from /opt/airflow/dags/crawl_data.py
[2025-01-10T18:54:19.176+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:54:19.175+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-01-10T18:54:19.213+0000] {logging_mixin.py:190} INFO - [2025-01-10T18:54:19.213+0000] {dag.py:4180} INFO - Setting next_dagrun for topcv_scraper to 2025-01-10 00:00:00+00:00, run_after=2025-01-11 00:00:00+00:00
[2025-01-10T18:54:19.243+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/crawl_data.py took 0.732 seconds
